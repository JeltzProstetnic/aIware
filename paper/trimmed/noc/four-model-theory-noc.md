# The Four-Model Theory of Consciousness: A Simulation-Based Framework Unifying the Hard Problem, Binding, and Altered States

**Matthias Gruber**

*Independent researcher*

*ORCID: 0009-0005-9697-1665*

*Correspondence: matthias@matthiasgruber.com*

---

## Abstract

No existing theory of consciousness simultaneously addresses eight core requirements a complete theory must meet: the Hard Problem, the Explanatory Gap, the Boundary Problem, the Structure of Experience, Unity and Binding, Combination and Emergence, the Causal Role, and the Meta-Problem. I present the Four-Model Theory, in which consciousness is constituted by real-time self-simulation across four nested models arranged along two axes — scope (world vs. self) and mode (implicit vs. explicit). The implicit models are substrate-level, learned, and non-conscious; the explicit models are virtual, transient, and phenomenal. The central claim is that qualia are virtual: they are the way the simulated self perceives its own states within the simulation, not properties of the physical substrate. This dissolves the Hard Problem by revealing a category error — the physical processing does not feel; the simulation does. Combined with a criticality requirement (the substrate must operate at the edge of chaos), five principles — criticality, virtual qualia, a redirectable Explicit Self Model, variable implicit-explicit permeability, and virtual model forking — unify psychedelic phenomenology, anesthetic mechanisms, dream states, split-brain phenomena, dissociative identity disorder, and animal consciousness. A systematic comparison shows the theory addresses all eight requirements. Nine novel testable predictions include that ego dissolution content is controllable via sensory input and that psychedelics should alleviate anosognosia. The criticality requirement, derived from Wolfram's computational framework in 2015, converges with empirical criticality literature consolidated in 2025-2026 (Hengen & Shew, 2025; Algom & Shriki, 2026) — a decade-apart convergence from theoretical and empirical starting points.

**Keywords**: consciousness, hard problem, self-model, simulation, qualia, criticality, binding problem, altered states, psychedelics, substrate independence

---

## 1. Introduction

### 1.1 The Pre-Paradigm State of Consciousness Science

After three decades of intensive scientific investigation, consciousness research finds itself at an impasse. The field possesses no dominant paradigm in the Kuhnian sense (Kuhn, 1962), no agreed-upon methodology for linking subjective experience to objective measurement, and no theory that commands broad assent. Multiple frameworks compete for explanatory primacy — Integrated Information Theory (IIT; Tononi, 2004; Albantakis et al., 2023), Global Neuronal Workspace (GNW; Baars, 1988; Dehaene & Changeux, 2011), Higher-Order Theories (HOT; Rosenthal, 2005; Lau & Rosenthal, 2011), Predictive Processing (PP; Friston, 2010; Seth, 2021), Attention Schema Theory (AST; Graziano, 2013), Recurrent Processing Theory (RPT; Lamme, 2006, 2010), and others — yet none has established decisive empirical or theoretical superiority over its rivals.

Recent developments have deepened the crisis rather than resolving it. The COGITATE adversarial collaboration published equivocal results in *Nature* (COGITATE Consortium, 2025; protocol: Melloni et al., 2023): neither IIT nor GNW was fully confirmed, and the data favored posterior cortical involvement over either camp's predictions. A letter signed by over 100 researchers declared IIT pseudoscientific (IIT-Concerned et al., 2025), provoking fierce rebuttals (Tononi, Albantakis, et al., 2025) and methodological commentary (Gomez-Marin & Seth, 2025).

This paper argues that the impasse persists because no existing theory simultaneously addresses all of the fundamental requirements that a complete theory of consciousness must meet. Each theory excels on some requirements but remains silent on, or weak against, others. The field needs a framework that addresses the full set of challenges — not just neural correlates or access conditions, but the deep philosophical problems that make consciousness uniquely difficult.

### 1.2 What Would Count as Progress?

I propose that any theory claiming to provide a comprehensive account of consciousness must address eight core requirements, drawn from the philosophical and scientific literature. These requirements are not novel; each has been identified by previous authors as a central challenge. What is novel is the demand that a single theory address all eight simultaneously:

1. **The Hard Problem** (Chalmers, 1995) — Why does physical processing give rise to subjective experience?
2. **The Explanatory Gap** (Levine, 1983) — Why does the explanation of neural correlates feel incomplete?
3. **The Boundary Problem** (Bayne, 2010; Tononi, 2004) — Where does the conscious system end?
4. **The Structure of Experience** (Nagel, 1974) — How does physical processing produce richly structured experience?
5. **Unity and Binding** (Treisman & Gelade, 1980; Revonsuo, 1999) — How are distributed processes unified into coherent experience?
6. **Combination and Emergence** (Chalmers, 2016) — How do non-conscious elements combine to produce consciousness?
7. **The Causal Role** (Jackson, 1982) — Does consciousness do anything?
8. **The Meta-Problem** (Chalmers, 2018) — Why do we think there is a hard problem?

Section 2 develops each requirement in detail and surveys how existing theories fare against them. The remainder of the paper presents a theory — the Four-Model Theory — that, I argue, addresses all eight.

### 1.3 Overview and Historical Context

The Four-Model Theory was originally published in German as *Die Emergenz des Bewusstseins* (Gruber, 2015) and has been refined through a structured adversarial challenge process in 2026. The theory self-identifies as an intersection of Dennett's Multiple Drafts Model (Dennett, 1991), Metzinger's Self-Model Theory of Subjectivity (Metzinger, 2003, 2009), and neural network architecture. It is substrate-independent: the six-layer mammalian cortex is understood as an evolutionary implementation, not a requirement.

The theory proposes that consciousness consists of a real-time self-simulation running on an implicit (substrate-level) knowledge base. Qualia — the subjective "feel" of experience — are virtual: they exist within and are constitutive of the simulation, but do not exist at the substrate level. This two-level ontology dissolves the Hard Problem by showing it rests on a category error.

Combined with a criticality requirement derived independently from Wolfram's computational framework (Wolfram, 2002), the theory generates nine novel testable predictions and unifies phenomena across psychopharmacology, clinical neurology, sleep science, and comparative cognition under five principles.

The theory is offered as one model among several, contributing to humanity's collective search for an adequate account of consciousness. Every model carries inherent modeling error; the present theory is no exception. It is intended to complement existing frameworks — extending where they are incomplete, not displacing where they succeed.

The paper proceeds as follows. Section 2 establishes the eight requirements. Section 3 presents the Four-Model Theory. Sections 4 and 5 develop the philosophical commitments and the binding/criticality framework. Section 6 demonstrates the theory's explanatory range. Section 7 provides a systematic comparative analysis against major competitors. Section 8 presents the nine predictions. Sections 9-11 address open questions, implications, and conclusions.

---

## 2. Eight Requirements for a Theory of Consciousness

Any theory claiming to provide a comprehensive account of consciousness must address eight core requirements. Each has been identified in the literature as a central challenge; what is novel is the demand that a single theory address all eight simultaneously. A detailed theory-by-theory comparison follows in Section 7.

### 2.1 The Hard Problem

Chalmers (1995, 1996) formulated the Hard Problem as the question of why physical processing is accompanied by subjective experience — why there is "something it is like" (Nagel, 1974) to undergo these processes. The challenge is not about identifying neural correlates but about understanding why correlates are accompanied by phenomenality at all.

### 2.2 The Explanatory Gap

Levine (1983) identified the Explanatory Gap: even if we identify every neural correlate of every conscious state, the explanation seems to leave something out. Block (1995, 2007) refined this as the distinction between access consciousness (functional role) and phenomenal consciousness (subjective feel). The Gap is about the *form* of explanation rather than the *existence* of the phenomenon.

### 2.3 The Boundary Problem

Where does the conscious system begin and end? Within the brain, only some processing is conscious at any given moment; between organisms, it is unclear where to draw the line (Bayne, 2010; Tononi, 2004; Friston, 2010; Bruineberg et al., 2022).

### 2.4 The Structure of Experience

Conscious experience has rich spatial, temporal, modal, and qualitative structure. Any complete theory must explain how physical processing generates this structured phenomenology.

### 2.5 Unity and Binding

The Binding Problem (Treisman & Gelade, 1980; Treisman, 1996; Revonsuo, 1999) asks how distributed neural processes — occurring in different brain regions, at different timescales, in different modalities — are unified into coherent experience. Proposed solutions range from temporal synchrony (Gray et al., 1989; Singer & Gray, 1995; Fries, 2005, 2015) to integrated information (Tononi, 2004) to global broadcasting (Baars, 1988). None is universally accepted.

### 2.6 Combination and Emergence

How do non-conscious elements combine to produce consciousness? For panpsychist theories (Goff, 2019; Strawson, 2006), this takes the form of the Combination Problem (James, 1890; Chalmers, 2016). For physicalist theories, the challenge is emergence: at what point does consciousness emerge from non-conscious processes? The Combination Problem is widely regarded as panpsychism's most serious difficulty (Chalmers, 2016; Coleman, 2014). A satisfactory theory must navigate between strong emergence difficulties (Kim, 1993) and this problem.

### 2.7 The Causal Role of Consciousness

Does consciousness *do* anything, or is it epiphenomenal (Huxley, 1874; Jackson, 1982)? Mechanistic theories struggle to specify what role *experience* plays beyond mechanism.

### 2.8 The Meta-Problem

Why do we *think* there is a Hard Problem (Chalmers, 2018)? Even if the Hard Problem is illusory, the fact that most humans report a strong intuition that consciousness is mysterious requires explanation.

---

## 3. The Four-Model Theory

### 3.1 Core Definition

Consciousness is the ability of an entity — biological or artificial — to create a model of itself, to relate that model to itself, and to interact with it. Consciousness is not a property the brain possesses but a process the brain performs: it runs a real-time self-simulation.

This definition is functional and substrate-independent. It does not require a specific physical implementation, biological composition, or computational architecture. What it requires is a system capable of constructing and maintaining a self-referential simulation in real time.

### 3.2 The Four Models

The theory identifies four nested models distinguished by two orthogonal dimensions: **scope** (everything vs. self only) and **mode** (implicit/learned vs. explicit/simulated). This is a conceptual taxonomy, not a claim about spatial organization in the brain — the models are functionally distinct processes, not anatomically localized regions (Figure 1).

**Table 1. The Four-Model Architecture**

| | Everything (world) | Self only |
|---|---|---|
| **Implicit** (learned, substrate-level) | Implicit World Model (IWM) | Implicit Self Model (ISM) |
| **Explicit** (simulated, phenomenal) | Explicit World Model (EWM) | Explicit Self Model (ESM) |

<img src="figure1-four-model-architecture.png" alt="Figure 1. The Four-Model Architecture" width="100%">

**The Implicit World Model (IWM)** encompasses the substrate's total accumulated knowledge about the world, stored in synaptic weights (or their functional equivalent in non-biological substrates). It includes everything the system has ever learned: perceptual regularities, causal models, spatial relationships, semantic knowledge, motor programs for interacting with the world. The IWM is never directly conscious. It operates "in the dark" — providing the knowledge base from which the conscious simulation is generated, but never itself appearing in experience.

**The Implicit Self Model (ISM)** is the substrate's accumulated self-knowledge: body schema, proprioceptive calibration, motor skills, habits, personality traits, autobiographical memory structures, and social self-knowledge. Like the IWM, the ISM is never directly conscious. There is no unified homunculus — no inner observer reading the ISM. The ISM is a structural feature of the substrate, not an experiential one.

**The Explicit World Model (EWM)** is the conscious world — the real-time simulation of reality that constitutes perceptual experience. When you see a room, hear a voice, feel the texture of a surface, you are experiencing the EWM. It is generated from the IWM (which provides the world-knowledge) and current sensory input (which constrains and updates the simulation), but it is not identical to either. The EWM is a virtual construct — a transient pattern of activity, not a permanent structure.

**The Explicit Self Model (ESM)** is the conscious self — the real-time simulation of "I" that constitutes self-experience. It is the sense of being a subject, having a perspective, occupying a body, possessing a history, and being the author of one's actions. The ESM is generated from the ISM (which provides the self-knowledge) and current interoceptive and proprioceptive input, but like the EWM, it is virtual: a transient process, not a permanent entity.

### 3.3 The Real/Virtual Split

The four models divide into two fundamental categories:

**The real side** (IWM + ISM): These are physical, structural, learned, and non-conscious. They are stored in the substrate's architecture — in biological brains, primarily in synaptic weights, dendritic morphology, and connectivity patterns. They accumulate over the organism's lifetime through learning. They have no phenomenal character. "Lights off."

**The virtual side** (EWM + ESM): These are simulated, transient, generated, and phenomenal. They are patterns of activity — in biological brains, transient electrochemical dynamics. They are constructed in real time from the implicit models and current sensory input. They *are* experience. "Lights on."

This division is the foundation of the theory's treatment of the Hard Problem (Section 3.4) and structures its account of every phenomenon it addresses (Figure 2).

The virtual models possess **software-like properties** that follow from their nature as simulations rather than structures:

- **They can be forked**: A single substrate can run multiple configurations of the ESM (see Section 6.2 on dissociative identity disorder).
- **They can be cloned**: Physical separation of the substrate produces degraded but complete copies of the virtual models (see Section 6.4 on split-brain).
- **They can be redirected**: The ESM requires input; disrupt normal self-referential input and it latches onto whatever input dominates (see Section 6.1 on psychedelics).
- **They can be reconfigured**: Therapeutic interventions (CBT, exposure therapy) work by modifying the virtual models through substrate-level rewiring (see Section 6.6).

<img src="figure2-real-virtual-split.png" alt="Figure 2. The Real/Virtual Split" width="100%">

### 3.4 Virtual Qualia: Dissolving the Hard Problem

The central claim of the Four-Model Theory is that **qualia are virtual**. They are the way the simulated self (ESM) perceives its own states and the simulated world (EWM). Qualia exist within and are constitutive of the simulation; they do not exist at the substrate level.

This dissolves the Hard Problem by revealing a category error in its formulation:

**The standard formulation**: "Why does physical processing (neuronal firing, synaptic transmission) feel like something?"

**The dissolution**: The physical processing *does not* feel like anything. The IWM and ISM — the substrate-level implicit models — operate without any phenomenal character whatsoever. There is nothing it is like to be a synaptic weight. The simulation, however, *does* feel — and within the simulation, qualia are simply what self-perception produces. Asking why neuronal firing feels like something is analogous to asking why transistor switching feels like running a video game. The transistors do not run the game at the level of individual switching; the virtual machine does. The neurons do not experience redness at the level of individual firing; the simulation does, and within the simulation, "redness" is simply the ESM's mode of registering a particular class of EWM content.

**Why self-simulation specifically?** A critic might object that this merely relocates the Hard Problem: why does *this* virtual process have experience when a weather simulation does not? The answer lies in **self-referential closure**. A weather simulation models weather; it does not model *itself modeling weather*. The four-model architecture creates a closed loop: the ESM is the system modeling its own modeling process. In this loop, the distinction between the model and the modeled collapses — the simulation *is* the thing being simulated. Qualia are not an *addition* to the self-modeling; they are the self-modeling as encountered from the inside of the loop. A non-self-referential simulation has an outside from which it can be described without remainder; a self-referential simulation at criticality has no such outside. The simulation *is* its own observer, and observation-from-inside is what we call experience.

This is not a proof that self-referential simulation must be conscious — it is an argument that self-referential simulation is the *kind* of process for which the Hard Problem's assumptions break down. Self-referential closure is precisely the condition under which the gap between process and feeling does not exist.

This is **not** illusionism in the sense of Dennett (1991) or Frankish (2016); nor is it compatible with deflationary accounts (Graziano, 2024). Illusionism holds that qualia as traditionally conceived are illusions — there is nothing it is like, and our sense that there is something it is like is itself a misrepresentation. The Four-Model Theory holds that qualia are *real within the simulation*. Within the EWM/ESM, experience has genuine phenomenal character. What is illusory is the assumption that this phenomenal character must be a property of the physical substrate. It is not. It is a property of the virtual process that the substrate runs.

This constitutes a **two-level ontology**: the substrate level (real side) has no experience, and the simulation level (virtual side) has genuine experience. Both levels are physical — the simulation is a physical process, not a supernatural one — but they have different ontological properties. The category error in the Hard Problem consists in conflating the two levels: seeking phenomenal properties at the substrate level where they do not exist.

The Explanatory Gap closes simultaneously. The gap between "neurons fire in pattern X" and "I experience red" reflects the level distinction: the firing pattern generates the simulation in which redness is experienced, but is not itself red, just as a CPU's electrical states are not "a spreadsheet" even though they generate one.

### 3.5 Graduated Levels of Consciousness

Consciousness is not binary but graduated, based on the depth of recursive self-modeling:

**Basic consciousness**: Minimal EWM and rudimentary ESM — phenomenal experience exists but self-awareness is thin. This is the entry level.

**Simply extended consciousness**: First-order self-observation. The system models itself — the ESM includes a model of the system's own states. The organism not only experiences but is aware that it experiences.

**Doubly extended consciousness**: Second-order self-observation. The system models itself modeling itself, enabling metacognition and the sense of being an observer of one's own mental processes.

**Triply extended consciousness**: Third-order self-observation. The system models itself modeling itself modeling itself. This supports the deepest forms of self-awareness, philosophical reflection, and the very intuition that consciousness is mysterious (connecting to the Meta-Problem — see Section 3.8). Notably, triply extended consciousness is also a prerequisite for the scientific study of consciousness itself: only a system capable of modeling its own modeling of its own experience can formulate the question "What is consciousness?"

Each level corresponds to an additional layer of recursive self-modeling. The levels are not discrete stages but points along a continuum. Different organisms — and potentially different artificial systems — occupy different positions along this continuum, and individual organisms may fluctuate between levels depending on state (waking, dreaming, meditative, intoxicated).

### 3.6 The Implicit-Explicit Boundary

A key mechanism in the Four-Model Theory is the **permeability of the boundary between implicit models (IWM/ISM) and explicit models (EWM/ESM)**. Information becomes conscious when it is transferred from the implicit to the explicit side — when substrate-level knowledge or self-knowledge is incorporated into the running simulation.

In normal waking states, this boundary is **selectively permeable**: relevant information passes through based on attentional and contextual gating. You are not conscious of everything your IWM knows about the world or everything your ISM knows about yourself; you are conscious of what the current simulation requires. Crucially, the boundary is not perfectly opaque even in normal states. Substrate-level processing artifacts routinely leak through: blind-spot filling (the visual system interpolating content where the optic nerve exits the retina), phosphenes from mechanical pressure on the eye, and visual snow phenomena all represent moments where processing-level activity becomes visible within the simulation. These normal-state leaks are subtle, but they demonstrate that the implicit-explicit boundary is a *graded* filter, not a wall — and they provide everyday evidence that conscious experience is a simulation generated from substrate processing rather than a direct window onto reality.

The permeability of this boundary is variable (Figure 3), and its variation explains a wide range of phenomena (detailed in Section 6): psychedelic states involve global permeability increase (intermediate processing stages become accessible); anosognosia involves local permeability decrease (the ISM contains deficit information but transfer to the EWM is blocked); pre-sleep states involve gradually increasing permeability (producing the same visual progression as psychedelics); and meditation involves trained modulation of permeability.

<img src="figure3-phenomenological-content.png" alt="Figure 3. Phenomenological Content Through a Morning" width="100%">

### 3.7 The Criticality Requirement

The Four-Model Theory imposes a **physical prerequisite** for consciousness: the substrate must operate at or near the edge of chaos — Wolfram's Class 4 computational regime (Wolfram, 2002).

Wolfram classified cellular automata (and by extension computational systems generally) into four classes:
- **Class 1**: Converges to a fixed state. Too simple for consciousness.
- **Class 2**: Periodic/repetitive. Too simple for consciousness.
- **Class 3**: Chaotic/random. Too disordered for coherent consciousness.
- **Class 4**: Complex/edge of chaos. Capable of universal computation. The regime in which consciousness can emerge.

This classification was applied to the question of consciousness in Gruber (2015), where it was argued that consciousness requires Class 4 dynamics — complex enough to sustain a self-simulation, ordered enough for that simulation to be coherent. This requirement was derived *theoretically*, from the computational properties needed for real-time self-modeling, not from empirical neuroscience.

Independently, empirical neuroscience has converged on the same conclusion through a different path. Beggs and Plenz (2003) demonstrated neuronal avalanches consistent with self-organized criticality in cortical tissue. Carhart-Harris et al. (2014) proposed the Entropic Brain Hypothesis, linking consciousness level to neural entropy. Tagliazucchi et al. (2012, 2016) showed criticality signatures in waking fMRI and under LSD. Priesemann et al. (2013, 2014) characterized brain dynamics as slightly subcritical in normal waking states. This line of research was formally consolidated in the Consciousness and Criticality (ConCrit) framework (Algom & Shriki, 2026), which synthesized evidence from 140 datasets across multiple paradigms to establish that consciousness tracks criticality across pharmacological, pathological, and physiological state changes.

**Table 2. Independent Convergence on Criticality**

| Year | Development | Path |
|------|------------|------|
| 2002 | Wolfram publishes *A New Kind of Science* | Computational theory |
| 2003 | Beggs & Plenz — neuronal avalanches, self-organized criticality | Empirical neuroscience |
| 2014 | Carhart-Harris — Entropic Brain Hypothesis | Empirical neuroscience |
| **2015** | **Gruber — Class 4 / edge of chaos requirement for consciousness** | **Theoretical (via Wolfram)** |
| 2016 | Tagliazucchi et al. — LSD and criticality | Empirical neuroscience |
| 2022 | "Self-organized criticality as a framework for consciousness" (review) | Empirical neuroscience |
| 2025 | Hengen & Shew — meta-analysis of 140 datasets confirms criticality | Empirical neuroscience |
| **2025-26** | **ConCrit framework (Algom & Shriki) — criticality as unifying mechanism for consciousness theories** | **Theoretical/empirical synthesis** |

While empirical work on neural criticality was already underway (Beggs & Plenz, 2003; Tagliazucchi et al., 2012), Gruber's (2015) derivation proceeded from Wolfram's computational universality framework rather than from neuroimaging data, representing genuine theoretical convergence with the empirical program. The later large-scale consolidation (Hengen & Shew, 2025; Algom & Shriki, 2026) confirmed the criticality-consciousness link across 140 datasets. This convergence — a theoretical prediction derived independently from computational first principles, later confirmed by large-scale empirical synthesis — provides notable support for one of the theory's core claims.

The Four-Model Theory distinguishes two thresholds for consciousness:
- **Physical threshold**: Criticality. The substrate must operate at Class 4 dynamics. Below this, no consciousness is possible regardless of architecture.
- **Functional threshold**: Four-model architecture. The substrate must implement the four-model self-simulation. Above criticality but without the architecture, there is complex dynamics but no consciousness.

Both thresholds must be met. Criticality is necessary but not sufficient; the four-model architecture is necessary but not sufficient. Together they are sufficient.

### 3.8 The Meta-Problem Dissolved

The Meta-Problem — why we think there is a Hard Problem — receives a natural account within the Four-Model Theory. The ISM (Implicit Self Model) is **structurally inaccessible** to the ESM (Explicit Self Model). The conscious self cannot directly observe its own substrate. When the ESM attempts to model the basis of its own experience, it encounters a principled opacity: the implicit models that generate the simulation are not themselves part of the simulation.

This is why consciousness *seems* mysterious. The ESM can represent that it is having an experience, but it cannot represent the mechanism by which the experience is generated — because that mechanism operates at the implicit/substrate level, which is by definition outside the explicit/virtual level. The result is the persistent intuition that something is being "left out" of any physical explanation: the ESM cannot find the mechanism within its own simulation, so it concludes the mechanism must be non-physical or fundamentally inexplicable.

The variable permeability of the implicit-explicit boundary (Section 3.6) adds a further dimension to this account. The boundary is not perfectly opaque: substrate-level processing artifacts occasionally leak through to the simulation — in altered states dramatically so, but even in normal waking states subtly. The conscious self thus inhabits a strange epistemic position: mostly sealed off from its own generative machinery, yet occasionally catching fleeting glimpses of something operating beneath the surface of experience. This architectural feature — near-opacity punctuated by occasional leaks — produces precisely the phenomenology that the Meta-Problem describes: the persistent, nagging intuition that consciousness is somehow deeper than any explanation can reach, that something vast operates just beyond the edge of introspective access. The mystery of consciousness is not evidence against the theory; it is a *prediction* of the theory. A virtual process with a mostly-opaque but imperfect boundary to its own substrate would experience exactly this sense of irreducible depth.

This account shares features with Graziano's AST explanation — both invoke the self-model's necessary incompleteness — but grounds it in a more specific architecture (four models, real/virtual split, variable permeability) and connects it to the broader dissolution of the Hard Problem rather than treating the Meta-Problem in isolation.

---

## 4. Philosophical Commitments

The Four-Model Theory entails specific philosophical positions that were established through structured adversarial analysis and are internally consistent. This section develops and defends each commitment.

### 4.1 Process Physicalism

The theory is physicalist: both the substrate (implicit models) and the simulation (explicit models) are physical processes. There is no non-physical substance, no fundamental experiential property, no panpsychist micro-experience. Qualia are higher-order physical patterns — specifically, they are patterns of activity within the simulation that constitute the ESM's self-perception within the EWM.

This is process physicalism rather than identity theory: consciousness is not identical to any particular neural state but is constituted by the *process* of self-simulation. The same conscious state could, in principle, be realized by different physical substrates — what matters is the functional architecture (four models at criticality), not the specific material.

Process physicalism avoids the difficulties of both type-identity theory (which struggles with multiple realization) and functionalism as traditionally conceived (which struggles with the Hard Problem). The Four-Model Theory adds the real/virtual level distinction to standard functionalism, which is what allows it to address phenomenality: qualia are not just functional roles but virtual properties of the simulation. They are real *as virtual properties* — genuinely experiential but not properties of the substrate.

### 4.2 Consciousness as Process, Not Agent

A persistent source of confusion in consciousness studies is the treatment of consciousness as an entity — an "it" that either does or does not cause things. The Four-Model Theory rejects this framing. Consciousness is not a thing; it is a process *performed* by the substrate. Asking whether consciousness "causes" anything is a category error — analogous to asking whether the pointer of a clock meeting the numerals causes the clock to work. The energy source drives the gears, which drive the pointer, but nowhere does the virtual interaction between pointer and numeral cause anything mechanical. Yet without that interaction the clock cannot be said to function — or malfunction.

The implicit models generate the virtual simulation for concrete adaptive reasons: the EWM integrates multimodal sensory data into a unified scene; the ESM provides a self-model against which consequences can be evaluated. This relationship is best understood as a **dual evaluation architecture**. The primary direction is that the implicit system actively *deploys* the virtual simulation as its evaluation mechanism: it presents decisions, actions, and their consequences to the simulation so that the simulation can assess outcomes, run scenarios, and register hedonic valence. But the explicit models also evaluate independently, albeit with significantly less computational bandwidth. The conscious simulation operates at approximately 20 Hz with a processing delay of roughly 500 ms (Van Rullen & Koch, 2003), while the substrate processes at vastly higher throughput. Over time, these conscious evaluations feed back to reshape the implicit models through learning. The result is two-way traffic: the implicit system uses the explicit as an evaluation tool (primary), and the explicit system contributes its own assessments back (secondary), shaping the substrate that generates it. The virtual simulation is the substrate's mechanism for consequence-observation and future-oriented adaptation — the very thing natural selection shaped the architecture to do.

This functional essentiality extends specifically to phenomenality. The substrate deploys the simulation as its evaluation mechanism — presenting situations so the simulation can assess consequences and register outcomes. For this evaluation to work, simulated states must have valence: they must *matter* to the simulation. A pain signal that is merely a numerical value without aversive character cannot drive avoidance at the simulation level. Only a simulation whose states carry hedonic valence — whose representations of threat, opportunity, and consequence are genuinely aversive or attractive — can perform the evaluative function that justifies the metabolic cost of maintaining the simulation. Phenomenality is not an optional feature of the simulation; it is the mechanism by which the simulation evaluates. Remove phenomenality and the simulation becomes a passive model rather than an evaluation engine — a spreadsheet rather than a simulation.

This makes the theory's position distinct from classical epiphenomenalism, in which consciousness is a causally inert by-product with no functional role. In the Four-Model Theory, the virtual models are in continuous feedback with the implicit models: the simulation's outputs feed back to update implicit processing, shaping future behavior. Qualia, as constitutive elements of that simulation, lack independent causal power over the substrate — much as the hands and numerals of a clock have no direct mechanical relation to the gear train, yet the clock cannot function as a clock without them. Remove the display and the mechanism still runs, but it no longer serves its purpose.[^quantum]

[^quantum]: As a corollary, consciousness plays no role in quantum measurement or wavefunction collapse. Observer-dependent interpretations of quantum mechanics (von Neumann, 1932; Wigner, 1961) are rejected. This is consistent with the dominant position in contemporary physics (decoherence approaches; Zurek, 2003) and with the theory's commitment to physicalism.

The theory reframes the free will debate. The ESM narrates decisions already made at the substrate level (Libet, 1985; Schurger et al., 2012; Wegner, 2002). The Four-Model Theory suggests that the substrate's continuous optimization of the organism's existence *is* the individual's will — merely not fully transparent to the ESM.

Standard objections are addressed: **Zombies** (Chalmers, 1996) are not possible — the virtual models *are* the substrate's activity at the virtual level; a system implementing the four-model architecture at criticality necessarily has phenomenal character. **The knowledge argument** (Jackson, 1982): Mary gains acquaintance with a virtual quale she could not access from substrate descriptions. **The evolutionary argument**: natural selection targets the functional capabilities of the architecture; the phenomenal character of the simulation is constitutive of those capabilities, not a free-rider.

### 4.3 Weak Emergence

Consciousness is weakly emergent: deducible in principle from a complete substrate description, even if practically irreducible. No strong emergence, no magical threshold. This avoids both strong emergence difficulties (Kim, 1993) and the panpsychist Combination Problem (Chalmers, 2016). Consciousness arises from the computational properties of a system running a self-simulation at criticality, as a weather pattern arises from atmospheric thermodynamics — no extra ingredient needed.

### 4.4 Substrate Independence

The six-layer mammalian neocortex is an evolutionary implementation of the four-model architecture, not a requirement for it. Consciousness is substrate-independent: any physical system capable of implementing the four-model architecture at criticality should produce consciousness.

The six-layer cortical architecture nonetheless provides a suggestive clue about the computational requirements for self-modeling. Universal approximation theory establishes that a three-layer neural network (input, hidden, output) suffices for arbitrary function approximation (Cybenko, 1989; Hornik et al., 1989). The mammalian neocortex consistently employs six layers — approximately three layers beyond what is required for information processing *per se*. The Four-Model Theory interprets this architectural "surplus" as the substrate's overhead for self-modeling (Gruber, 2015): the additional layers provide the computational capacity needed to run the explicit models (EWM and ESM) as real-time simulations *on top of* the implicit processing that three layers would suffice for.

Biological evidence already supports this. Corvids (crows, ravens) and parrots demonstrate cognitive abilities — tool use, future planning, mirror self-recognition, social deception — that strongly suggest consciousness, yet their brains have no neocortex. Their pallium is organized in nuclear clusters rather than layers (Güntürkün & Bugnyar, 2016). Cephalopods (octopuses) demonstrate problem-solving, tool use, and behavioral flexibility with an even more radically different brain architecture — a largely decentralized nervous system with more neurons in the arms than in the central brain. If the Four-Model Theory is correct, these animals are conscious not because they share our neural architecture but because they have evolved functionally equivalent self-simulation architectures on different substrates — exactly what substrate independence predicts.

The implication for artificial consciousness is direct: a synthetic system implementing the four-model architecture at criticality should produce genuine consciousness (see Section 8, Prediction 7 and Section 10.1). Current AI systems, including large language models, do not meet this specification. LLMs lack an Explicit Self Model (they do not run a real-time self-simulation), lack criticality (transformer inference is a feedforward pass — Class 1/2 dynamics), and lack the real/virtual split that grounds phenomenality. The theory predicts that the qualitative difference between interacting with a genuinely conscious artificial system and interacting with an LLM would be immediately and qualitatively distinguishable.

---

## 5. Binding, Criticality, and Holographic Storage

### 5.1 Binding as an Emergent Property of Critical Dynamics

Binding is not a separate mechanism but an emergent property of criticality. At the edge of chaos, maximal correlation length means distant substrate regions influence each other and information integrates across the network. Binding is a consequence of the dynamical regime, not an additional computation. This is consistent with foundational work on oscillatory binding (Gray et al., 1989), long-distance neural synchronization (Rodriguez et al., 1999), gamma-band synchrony and temporal correlation (Singer & Gray, 1995; Fries, 2005, 2015), thalamocortical coherence in dream states (Llinás & Ribary, 1993) and waking (Llinás et al., 1998), and criticality signatures (Beggs & Plenz, 2003; Tagliazucchi et al., 2012).

### 5.2 Holographic Storage

The implicit models (IWM, ISM) store information in a distributed, non-local manner — standard distributed representations (Hinton et al., 1986) where each piece of information is spread across many connection weights and each weight participates in storing many pieces of information. This produces graceful degradation: partial damage reduces quality but does not eliminate specific memories or skills.

The term "holographic" is used as analogy: just as cutting a hologram in half produces two complete but lower-resolution images, splitting a distributed network produces two degraded but functionally complete copies. This property is critical for understanding split-brain phenomena (Section 6.4) and makes the specific prediction that callosotomy should produce bilateral but degraded function, not clean lateralized loss.

### 5.3 Consciousness States Derived from Criticality

The criticality requirement provides a unified account of when consciousness is present and when it is absent. Consciousness tracks the substrate's position relative to the critical point:

**Table 3. Consciousness States and Criticality**

| State | Criticality | Models | Consciousness | Evidence |
|-------|---|---|---|---|
| Normal waking | At/near critical | All four active | Full | High PCI, rich EEG |
| REM sleep | Near-critical, no input | EWM/ESM on internal input | Degraded (dream) | Moderate PCI |
| Deep NREM | Subcritical | EWM/ESM collapse | Absent | Low PCI, slow waves |
| Propofol | Forced subcritical | EWM/ESM suppressed | Absent | PCI ≈ 0 |
| Ketamine | Not subcritical (↑ entropy) | EWM/ESM on wrong input | Present, disconnected | ↑ EEG entropy |
| Psychedelics | At/past critical | All active, ↑ permeability | Present, altered | ↑ Complexity |
| Vegetative | Typically subcritical | EWM/ESM collapsed | Absent (usually) | Low PCI |
| Covert awareness | At critical, output damaged | EWM/ESM intact, no motor | Present, unexpressible | Owen (2006) |
| Minimally conscious | Fluctuating | Intermittent EWM/ESM | Intermittent | Fluctuating PCI |

The key distinction highlighted by this framework is between **propofol** and **ketamine**. Both are anesthetics, yet their phenomenology differs dramatically. Propofol produces absence: patients report no experience (Alkire et al., 2000; Boly et al., 2012). Ketamine produces the "K-hole" — vivid, often bizarre experiences of dissociation, out-of-body phenomena, and altered identity (Corlett et al., 2011). The Four-Model Theory predicts this difference: propofol pushes the substrate subcritical, abolishing the conditions for consciousness. Ketamine does *not* push subcritical — it increases neural entropy (Schartner et al., 2017) — but disrupts normal sensory input processing, causing the EWM and ESM to operate on internal and distorted signals. Consciousness is present but disconnected from external reality.

This is a genuine explanatory advantage. Most theories struggle to account for why two agents classified as "anesthetics" produce such radically different phenomenological profiles. The criticality framework makes the distinction natural: what matters is not pharmacological classification but the effect on the substrate's dynamical regime.

---

## 6. Explanatory Range

A theory's value lies partly in its ability to derive diverse phenomena from a small set of principles. The Four-Model Theory's five principles — criticality, virtual qualia, redirectable ESM, variable implicit-explicit permeability, and virtual model forking — generate accounts of phenomena across psychopharmacology, clinical neurology, sleep science, comparative cognition, and clinical psychology. This section demonstrates that range.

### 6.1 Psychedelic Phenomenology

Psychedelics produce visual intensification, synesthesia, enhanced pattern recognition, ego dissolution, and identity alteration (Carhart-Harris et al., 2012, 2016; Timmermann et al., 2019, 2023). The theory accounts for this through three mechanisms.

**Permeability increase**. Psychedelics increase the global permeability of the implicit-explicit boundary. Intermediate processing stages leak through, producing an ordered visual progression: V1-level (phosphenes) → V2/V3-level (form constants; Klüver, 1966) → higher areas (faces, scenes). This follows the visual hierarchy in a predictable, dose-dependent order.

**Ego dissolution = ESM redirection**. At high doses, the ESM loses normal self-referential input but continues to run, latching onto whatever dominates the available input stream. **Salvia divinorum** phenomenology dramatically confirms this: users reliably report "becoming" objects in their environment — the ESM, deprived of self-input, latches onto dominant sensory input. Few competing theories generate this prediction (Section 8, Prediction 3).

**Intensity as novelty**. Psychedelic profundity reflects increased *novel content*, not increased consciousness level.

### 6.2 Anesthesia and Clinical Disorders

**Propofol** pushes the substrate subcritical → consciousness abolished (Table 3). **Ketamine** does not push subcritical — it increases neural entropy (Schartner et al., 2017) — but disrupts sensory processing: consciousness present but disconnected (Corlett et al., 2011). What matters is not pharmacological classification but the effect on the substrate's dynamical regime.

**Covert awareness**: If the substrate is at criticality with damaged *output pathways*, consciousness is present but unexpressible — cognitive motor dissociation (Owen et al., 2006; Monti et al., 2010). The theory predicts this distinction should be detectable via criticality measures such as PCI (Casali et al., 2013; Casarotto et al., 2016).

**Cotard's delusion**: The ESM receives distorted interoceptive input and constructs "I am dead" as its best model — the same redirectable-ESM mechanism as salvia, applied clinically.

**Anosognosia**: A **local decrease in implicit-explicit permeability** — the ISM contains the deficit information but transfer to the EWM is blocked. This is the inverse of the psychedelic mechanism, connected under the single principle of variable permeability (Section 8, Prediction 4).

**DID**: The virtual models can be **forked**. Each alter is a distinct ESM configuration running on the same substrate, predicting distinct neural signatures detectable with neuroimaging (Section 8, Prediction 9).

### 6.3 Dreams

Dreaming represents the simulation in **degraded mode**: near-critical dynamics but with external input cut off. The EWM draws on the IWM's stored knowledge, producing familiar places, impossible physics, and emotional intensity; the ESM generates a self with reduced metacognitive oversight.

**Lucid dreaming** provides direct evidence for the software-like quality of the virtual models: the ESM "toggles on" more fully within the dream state. The theory predicts lucid dream onset corresponds to a **criticality threshold crossing** — a step-like discontinuity in EEG complexity (Section 8, Prediction 8). The NREM/REM transition likewise reflects the substrate oscillating around the critical point.

### 6.4 Split-Brain

Callosotomy (Gazzaniga, Bogen, & Sperry, 1962, 1965; Gazzaniga, 2000) produces **two degraded but functionally complete copies** of the four-model architecture, because the implicit models store information holographically (Section 5.2). Each hemisphere sustains independent consciousness above the criticality threshold. The left hemisphere interpreter (Gazzaniga, 2000) — confabulating explanations for the right hemisphere's behavior — is the *same* ESM mechanism observed in Cotard's and anosognosia. Graded rather than binary deficits (Pinto et al., 2017) are consistent with holographic degradation.

### 6.5 Animal Consciousness

The theory predicts a **gradient** of animal consciousness. Corvids and parrots — tool manufacture, mirror self-recognition, social deception — have no neocortex, with pallium organized in nuclear clusters (Güntürkün & Bugnyar, 2016). The theory predicts they are conscious because they have evolved functionally equivalent self-simulation architectures, directly testing substrate independence (see also Section 4.4).

### 6.6 Clinical Psychology and Neurology

**CBT** works as virtual model reprogramming: repeated corrective experience drives substrate-level rewiring, modifying the ISM and thereby the ESM's self-model. **Phobias** are EWM misconfigurations; exposure therapy updates the IWM. **The placebo effect** illustrates the theory's causal architecture: placebo activates substrate-level expectation circuits in parallel with — not caused by — conscious hope. **Conversion disorder** inverts blindsight: the EWM models a deficit the intact substrate does not have.

**Blindsight** (Weiskrantz, 1986) provides the clearest demonstration that substrate-level processing can proceed without conscious representation — the IWM processes visual information through subcortical pathways while damaged cortical pathways fail to relay it to the EWM. **Anton's syndrome** (Anton, 1899; Aldrich et al., 1987) presents the precise inverse: the EWM generates visual simulation from stored knowledge despite absent input. Together, they constitute a double dissociation between substrate processing and conscious simulation — perhaps the most direct neurological evidence for the real/virtual distinction.

---

## 7. Comparative Analysis

This section provides a systematic comparison between the Four-Model Theory and six major competitors across the eight requirements established in Section 2. The comparison aims to be fair: each theory's genuine strengths are acknowledged, and the Four-Model Theory's advantages are located precisely.

### 7.1 Scoring Matrix

Table 4 presents an assessment of how each theory addresses the eight requirements. All ratings reflect the present author's judgment and are offered as a starting point for discussion, not as definitive verdicts. Readers are encouraged to consult the primary sources and form their own assessments.

**Assessment criteria**: *Addresses* = the theory provides a substantive, defended account of this requirement. *Partial* = the theory provides a relevant account that leaves significant aspects unresolved. *Minimal* = the theory touches on this requirement but does not develop a full treatment. *Silent* = the theory does not address this requirement (which may reflect deliberate scope limitation rather than failure). *N/A* = the requirement does not apply given the theory's ontological commitments.

**Table 4. Theory Comparison Across Eight Requirements**

Ratings: ● = addresses, ◐ = partial, ○ = minimal, — = silent, n/a = not applicable.

| Requirement | FMT | IIT | GNW | HOT | PP | AST | RPT |
|---|---|---|---|---|---|---|---|
| Hard Problem | ● | ●† | —* | ◐ | —* | ◐ | — |
| Expl. Gap | ● | ●† | —* | ◐ | —* | ◐ | — |
| Boundary | ● | ● | ◐ | ○ | ◐ | ◐ | ◐ |
| Structure | ● | ● | ◐ | ◐ | ● | ◐ | ◐ |
| Binding | ● | ● | ◐ | ○ | ◐ | ○ | ◐ |
| Combination | ● | ○†† | n/a | n/a | n/a | n/a | n/a |
| Causal Role | ● | ◐ | ◐ | ◐ | ● | ◐ | ● |
| Meta-Problem | ● | ○ | ◐ | ◐ | ◐ | ● | ○ |

† IIT addresses the Hard Problem through its axioms, identifying consciousness with integrated information (Φ). Whether this constitutes a solution or a redefinition is debated (see §7.2).
†† IIT's panpsychist commitments lead to the Combination Problem (Chalmers, 2016), which remains unresolved within the framework.
\* GNW and PP proponents argue these theories address the "real problem" of consciousness (Seth, 2021) — explaining the structure and contents of experience — even if they do not address the Hard Problem as Chalmers defines it. This is a legitimate methodological choice, not a deficiency; the "silent" rating reflects the scope of the requirement as defined in §2, not a judgment on the theories' overall merit.

### 7.2 Theory-by-Theory Comparison

**IIT** (Tononi, 2004; Albantakis et al., 2023): Strongest on mathematical rigor and qualia space, but its identification of consciousness with Φ leads to panpsychist consequences and the Combination Problem (Chalmers, 2016). Φ is computationally intractable (Aaronson, 2014) and the unfolding argument challenges the central claim (Doerig et al., 2019). The Four-Model Theory avoids panpsychism and generates predictions without computing Φ. **GNW** (Baars, 1988; Dehaene & Changeux, 2011): Clear on access consciousness but silent on the Hard Problem. COGITATE results (2025) favored posterior cortex over frontal predictions. **HOT** (Rosenthal, 2005; Lau & Rosenthal, 2011): Explains which states are conscious but not *why* higher-order representation produces phenomenality. **PP** (Seth, 2021): Strong on experiential structure but explicitly silent on the Hard Problem. **AST** (Graziano, 2013): Strongest Meta-Problem account but deflationary about phenomenality. **RPT** (Lamme, 2006, 2010): Empirically specific but limited to visual consciousness and silent on the Hard Problem. The Four-Model Theory adds the real/virtual distinction and criticality requirement that address phenomenality — the gap common to all competitors.

### 7.3 Emerging Frameworks and Summary

**Biological computationalism** (Milinkovic & Aru, 2025) challenges substrate independence; the Four-Model Theory treats this as empirical, noting that corvids already favor substrate independence. The **Multiple Generator Hypothesis** (Kirkeby-Hinrup, Fink, & Overgaard, 2025) is potentially compatible: the four models could be understood as distinct generators unified by criticality.

The theory's comparative advantages: (1) dissolving the Hard Problem without panpsychism or strong emergence; (2) unifying binding with criticality; (3) the redirectable ESM for identity-content prediction; (4) connecting psychedelics and anosognosia under variable permeability; (5) the Meta-Problem as structural consequence. Its primary disadvantage is the absence of mathematical formalization (Section 9).

---

## 8. Novel Testable Predictions

A theory is only as valuable as the predictions it generates. The Four-Model Theory yields nine novel testable predictions, several of which are highly distinctive.

### 8.1 Prediction 1: Distinct fMRI Signatures for Each Model

Tasks selectively engaging a single model should produce distinct neural activation patterns. IWM-dominant tasks (implicit priming) should activate substrate-level storage networks (hippocampal-cortical), ISM-dominant tasks (habitual motor sequences) somatosensory-cerebellar networks, EWM-dominant tasks (novel scene processing) sensory cortices, and ESM-dominant tasks (self-reflection) default mode and medial prefrontal cortex.

**Testability**: High — a factorial battery crossing scope × mode, with the prediction of double dissociation. **Unique**: Yes — the 2×2 structure is mandated only by this theory.

### 8.2 Prediction 2: Psychedelic Content Maps the Processing Hierarchy

Under psychedelics, visual content progresses through the cortical hierarchy in a dose-dependent sequence: V1 (phosphenes) → V2/V3 (form constants) → higher areas (faces, figures) → complex scenes. **Testability**: High (graded dosing + concurrent fMRI/MEG). Partial evidence exists (Carhart-Harris et al., 2016; Timmermann et al., 2023) but has not been systematically tested as dose-correlated progression. **Unique**: Partially — PP predicts hierarchical effects but not the specific ordered sequence.

### 8.3 Prediction 3: Ego Dissolution Content Is Controllable

During psychedelic ego dissolution, the content of the altered identity experience tracks the dominant sensory input. The ESM, deprived of normal self-referential input, latches onto whatever dominates the available stream. By controlling sensory input during ego dissolution, identity content can be predicted and directed.

**Testability**: High — administer ego-dissolution-inducing doses under controlled conditions, vary dominant sensory input, measure correspondence with reported identity content. **Unique**: **Yes — the theory's most distinctive prediction.** No competing theory specifies what a subject will "become" during ego dissolution.

### 8.4 Prediction 4: Psychedelics Alleviate Anosognosia

Sub-ego-dissolution doses of psychedelics should alleviate anosognosia by globally increasing implicit-explicit permeability, overwhelming the local block that causes deficit unawareness and allowing deficit information in the ISM to reach the EWM.

**Testability**: Medium (requires clinical trial). **Unique**: **Yes — a cross-domain surprise prediction.** No other theory connects psychedelics and anosognosia through a single mechanism.

### 8.5 Prediction 5: All Anesthetics Converge on Criticality Disruption

Despite diverse receptor mechanisms, all agents that abolish consciousness push the substrate below the criticality threshold; agents that alter but preserve consciousness (ketamine, psychedelics) do not.

**Testability**: High — measure criticality indicators across anesthetic agents at equi-potent doses. **Unique**: Shared with ConCrit (Algom & Shriki, 2026), but predicted from theoretical first principles (Gruber, 2015) prior to the empirical consolidation.

### 8.6 Prediction 6: Split-Brain Produces Holographic Degradation

After callosotomy, each hemisphere retains degraded but functionally *complete* cognitive and experiential capacities — not clean hemispheric specialization. Degradation should be proportional to commissural severing.

**Testability**: High. Pinto et al. (2017) provide preliminary evidence. **Unique**: Yes — predicts graded degradation proportional to disconnection, not binary split.

### 8.7 Prediction 7: Artificial Consciousness via Four Models at Criticality

A synthetic system implementing the four-model architecture at criticality will exhibit consciousness, qualitatively distinguishable from current LLMs (which lack ISM/ESM, criticality, and the real/virtual split). Partial implementations should show partial consciousness indicators.

**Testability**: Medium (requires engineering development). **Unique**: Yes — provides a specific architectural blueprint, unlike theories merely compatible with artificial consciousness.

### 8.8 Prediction 8: Lucid Dream Onset Is a Criticality Threshold Crossing

The transition from non-lucid to lucid dreaming corresponds to a step-like increase in neural complexity — a criticality threshold crossing — detectable as a discontinuity in EEG complexity measures (LaBerge, 1985).

**Testability**: High (established paradigms). **Unique**: Partially — the step-like discontinuity prediction is distinctive.

### 8.9 Prediction 9: DID Alters Have Distinct Neural Signatures

Different alters in DID correspond to distinct configurations of neural activity (virtual model forking), with differences localizing to ESM-related networks (default mode, medial prefrontal, posterior cingulate).

**Testability**: High — fMRI/EEG during controlled alter switching (cf. Reinders et al., 2003, 2008). **Unique**: Yes — predicts *consistent, alter-specific* neural signatures, not merely differences.

### 8.10 The Ultimate Prediction

If correct, it should be possible to *build* a conscious machine by implementing the four-model architecture at criticality. Such a system would not merely simulate consciousness but would *be* conscious. This prediction is not currently testable, but it sets a bold bar: the difference between interacting with a conscious artificial system and any current AI should be qualitatively obvious — a difference in *kind*, not degree.

---

## 9. Open Questions

Intellectual honesty requires identifying what the theory does not yet resolve. These are research frontiers, not theoretical weaknesses — they are questions that arise *from* the theory and that the theory's framework helps to sharpen.

**1. Are all four models virtual?** The theory describes the implicit models (IWM, ISM) as "real side" and the explicit models (EWM, ESM) as "virtual side." But it is not certain that this division is sharp. The implicit models might themselves have virtual properties — they are, after all, *models*, not raw physics. If the implicit models are also virtual in some sense, what constitutes the "real side"? This is an open question even for the theory's author, and its resolution may have consequences for the Hard Problem treatment.

**2. Mathematical formalization.** The criticality requirement is specified qualitatively (Wolfram's Class 4 regime), not quantitatively. A full mathematical treatment — defining the four models formally, specifying the criticality threshold in terms of measurable quantities, deriving the predictions as formal consequences — remains to be developed. ConCrit's mathematical tools (power-law exponents, detrended fluctuation analysis, branching parameters) provide a starting point, as do the formal tools of dynamical systems theory and information geometry.

**3. Physical implementation.** Which physical mechanism in the biological brain supports criticality? Candidates include cortical column dynamics, thalamocortical standing waves, glial modulation, and (more speculatively) quantum processes in microtubules (Penrose & Hameroff, 1994; though see Tegmark, 2000 for decoherence objections). The theory is agnostic: it specifies the *functional* requirements without mandating a specific physical mechanism.

**4. Minimum configuration.** Can the four models partially dissociate? Is it possible to have an EWM without an ESM (world-experience without self-experience), or an ESM without an EWM? What is the minimum set of models required for consciousness? The graduated levels (Section 3.5) suggest a hierarchy, but the exact minimum configuration may require simulation or empirical investigation to determine.

---

## 10. Discussion

### 10.1 Implications for Artificial Consciousness

The theory provides an engineering specification for artificial consciousness. Current LLMs fail on multiple counts: feedforward inference (Class 1/2 dynamics), no ISM/ESM, and no real/virtual split. This does not prove LLMs are non-conscious, but they lack the architecture the theory specifies. The growing discourse around AI consciousness (Butlin et al., 2023, 2025; Schwitzgebel, 2025; Birch, 2025) and AI welfare (Long et al., 2024; Anthropic, 2025) makes clear criteria for artificial consciousness practically important.

### 10.2 Implications for Consciousness Science

Rather than adjudicating between IIT and GNW, the field should: (1) test the criticality prediction across anesthetic agents (Prediction 5); (2) design controlled ego-dissolution experiments (Prediction 3); (3) investigate the psychedelic-anosognosia connection (Prediction 4); (4) measure criticality at lucid dream onset (Prediction 8). These experiments test specific mechanisms that could be incorporated into other frameworks if confirmed.

### 10.3 Limitations

**No institutional laboratory.** Empirical testing depends on established laboratories. **The causal status of experience occupies contested ground.** Qualia lack independent causal power yet the simulation is functionally essential (Section 4.2) — distinct from classical epiphenomenalism, but facing resistance from both strong causalists and strict epiphenomenalists. **Qualitative rather than quantitative.** Formalization is a priority for future work. **The other-minds problem** applies to the ultimate prediction. **Inherent limits of inside-modeling.** Analogous to Godel's incompleteness: the brain modeling itself faces an irreducible epistemological gap, acknowledged through the Meta-Problem (Section 3.8). **Every model has modeling error.** The theory claims to be useful, not final; the predictions in Section 8 are designed to reveal where it is wrong.

---

## 11. Conclusion

The Four-Model Theory proposes that consciousness is a real-time self-simulation across four nested models operating at the edge of chaos. Qualia are virtual: phenomenal properties of the simulation, not of the substrate. This dissolves the Hard Problem by revealing a category error, simultaneously closing the Explanatory Gap and accounting for the Meta-Problem. The theory addresses all eight requirements established in Section 2.

Nine novel testable predictions include that ego dissolution content is controllable via sensory input (Prediction 3), that psychedelics should alleviate anosognosia (Prediction 4), and that all consciousness-abolishing anesthetics converge on criticality disruption (Prediction 5). The criticality requirement, derived from Wolfram's framework in 2015, was independently confirmed by the later empirical consolidation (Hengen & Shew, 2025; Algom & Shriki, 2026).

The theory offers a direct answer to why there is experience: there is experience because there is a self-simulation, and within the simulation, experience is constitutive of the process. The way to test this answer is through the predictions it generates — and ultimately, through building a system to the specification. The theory's implications extend beyond consciousness science: the cognitive-learning capacity it identifies as a consequence of the four-model architecture is the foundation for a recursive model of intelligence (Gruber, forthcoming).

---

## Acknowledgments

This paper was written with the assistance of Claude (Anthropic), which served as editor, cross-checker, and writing tool throughout the drafting process. The theory, arguments, predictions, and all intellectual content are entirely the author's, originally published in Gruber (2015); Claude assisted with literature cross-referencing, prose drafting from the author's directions, and internal consistency checking.

---

## Data Availability

No new data were generated or analysed in support of this research.

---

## Funding

This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.

---

## Conflict of Interest

The author declares no conflict of interest.

---

## Author Contributions

**Matthias Gruber**: Conceptualization, Investigation, Methodology, Writing — Original Draft, Writing — Review & Editing, Visualization.

---

## References

Aaronson, S. (2014). Why I am not an integrated information theorist. Blog post. *Shtetl-Optimized*.

Albantakis, L., Barbosa, L., et al. (2023). Integrated information theory (IIT) 4.0: Formulating the properties of phenomenal existence in physical terms. *PLOS Computational Biology*, 19(10), e1011465.

Aldrich, M.S., Vanderzant, C.W., Alessi, A.G., Abou-Khalil, B., & Sackellares, J.C. (1987). Cortical blindness: Etiology, diagnosis, and prognosis. *Annals of Neurology*, 21(2), 149-158.

Algom, I. & Shriki, O. (2026). The concrit framework: Critical brain dynamics as a unifying mechanistic framework for theories of consciousness. *Neuroscience & Biobehavioral Reviews*, 180, 106483.

Alkire, M.T., Haier, R.J., & Fallon, J.H. (2000). Toward a unified theory of narcosis: Brain imaging evidence for a thalamocortical switch as the neurophysiologic basis of anesthetic-induced unconsciousness. *Consciousness and Cognition*, 9(3), 370-386.

Anthropic. (2025). Exploring model welfare. Research report.

Anton, G. (1899). Über die Selbstwahrnehmung der Herderkrankungen des Gehirns durch den Kranken. *Archiv für Psychiatrie und Nervenkrankheiten*, 32, 86-127.

Baars, B.J. (1988). *A Cognitive Theory of Consciousness*. Cambridge University Press.

Bayne, T. (2010). *The Unity of Consciousness*. Oxford University Press.

Beggs, J.M. & Plenz, D. (2003). Neuronal avalanches in neocortical circuits. *Journal of Neuroscience*, 23(35), 11167-11177.

Birch, J. (2025). AI consciousness: A centrist manifesto. *PhilPapers*.

Block, N. (1995). On a confusion about a function of consciousness. *Behavioral and Brain Sciences*, 18(2), 227-247.

Block, N. (2007). Consciousness, accessibility, and the mesh between psychology and neuroscience. *Behavioral and Brain Sciences*, 30(5-6), 481-499.

Boly, M., et al. (2012). Connectivity changes underlying spectral EEG changes during propofol-induced loss of consciousness. *Journal of Neuroscience*, 32(20), 7082-7090.

Bruineberg, J., Dolega, K., Dewhurst, J., & Baltieri, M. (2022). The Emperor's new Markov blankets. *Behavioral and Brain Sciences*, 45, e183.

Butlin, P., et al. (2023). Consciousness in artificial intelligence: Insights from the science of consciousness. *arXiv*:2308.08708.

Butlin, P., et al. (2025). Identifying indicators of consciousness in AI systems. *Trends in Cognitive Sciences*.

Carhart-Harris, R.L., et al. (2012). Neural correlates of the psychedelic state as determined by fMRI studies with psilocybin. *Proceedings of the National Academy of Sciences*, 109(6), 2138-2143.

Carhart-Harris, R.L., et al. (2014). The entropic brain: A theory of conscious states informed by neuroimaging research with psychedelic drugs. *Frontiers in Human Neuroscience*, 8, 20.

Carhart-Harris, R.L., et al. (2016). Neural correlates of the LSD experience revealed by multimodal neuroimaging. *Proceedings of the National Academy of Sciences*, 113(17), 4853-4858.

Casali, A.G., et al. (2013). A theoretically based index of consciousness independent of sensory processing and behavior. *Science Translational Medicine*, 5(198), 198ra105.

Casarotto, S., et al. (2016). Stratification of unresponsive patients by an independently validated index of brain complexity. *Annals of Neurology*, 80(5), 718-729.

Chalmers, D.J. (1995). Facing up to the problem of consciousness. *Journal of Consciousness Studies*, 2(3), 200-219.

Chalmers, D.J. (1996). *The Conscious Mind: In Search of a Fundamental Theory*. Oxford University Press.

Chalmers, D.J. (2016). The combination problem for panpsychism. In G. Brüntrup & L. Jaskolla (Eds.), *Panpsychism: Contemporary Perspectives*. Oxford University Press.

Chalmers, D.J. (2018). The meta-problem of consciousness. *Journal of Consciousness Studies*, 25(9-10), 6-61.

COGITATE Consortium. (2025). An adversarial collaboration to critically evaluate theories of consciousness. *Nature*.

Coleman, S. (2014). The real combination problem: Consciousness, panpsychism, and phenomenal bonding. *Erkenntnis*, 79(S1), 19-44.

Corlett, P.R., et al. (2011). Glutamatergic model psychoses: Prediction error, learning, and inference. *Neuropsychopharmacology*, 36(1), 294-315.

Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. *Mathematics of Control, Signals and Systems*, 2(4), 303-314.

Dehaene, S. & Changeux, J.P. (2011). Experimental and theoretical approaches to conscious processing. *Neuron*, 70(2), 200-227.

Dennett, D.C. (1991). *Consciousness Explained*. Little, Brown and Company.

Doerig, A., et al. (2019). The unfolding argument: Why IIT and other causal structure theories cannot explain consciousness. *Consciousness and Cognition*, 72, 49-59.

Frankish, K. (2016). Illusionism as a theory of consciousness. *Journal of Consciousness Studies*, 23(11-12), 11-39.

Fries, P. (2005). A mechanism for cognitive dynamics: Neuronal communication through neuronal coherence. *Trends in Cognitive Sciences*, 9(10), 474-480.

Fries, P. (2015). Rhythms for cognition: Communication through coherence. *Neuron*, 88(1), 220-235.

Friston, K. (2010). The free-energy principle: A unified brain theory? *Nature Reviews Neuroscience*, 11(2), 127-138.

Gazzaniga, M.S. (2000). Cerebral specialization and interhemispheric communication: Does the corpus callosum enable the human condition? *Brain*, 123(7), 1293-1326.

Gazzaniga, M.S., Bogen, J.E., & Sperry, R.W. (1962). Some functional effects of sectioning the cerebral commissures in man. *Proceedings of the National Academy of Sciences*, 48(10), 1765-1769.

Gazzaniga, M.S., Bogen, J.E., & Sperry, R.W. (1965). Observations on visual processes after posterior callosal section. *Neurology*, 15(2), 97-106.

Goff, P. (2019). *Galileo's Error: Foundations for a New Science of Consciousness*. Pantheon Books.

Gomez-Marin, A. & Seth, A.K. (2025). A science of consciousness beyond pseudo-science and pseudo-consciousness. *Nature Neuroscience*, 28, 703-706.

Gray, C.M., König, P., Engel, A.K., & Singer, W. (1989). Oscillatory responses in cat visual cortex exhibit inter-columnar synchronization which reflects global stimulus properties. *Nature*, 338(6213), 334-337.

Graziano, M.S.A. (2013). *Consciousness and the Social Brain*. Oxford University Press.

Graziano, M.S.A. (2024). Illusionism big and small: Some options for explaining consciousness. *eNeuro*, 11(10), ENEURO.0210-24.2024.

Gruber, M. (2015). *Die Emergenz des Bewusstseins*. Self-published. ISBN 9781326652074.

Gruber, M. (forthcoming). Why intelligence models must include motivation: A recursive framework. Manuscript in preparation for *New Ideas in Psychology*.

Güntürkün, O. & Bugnyar, T. (2016). Cognition without cortex. *Trends in Cognitive Sciences*, 20(4), 291-303.

Hengen, K.B. & Shew, W.L. (2025). Is criticality a unified setpoint of brain function? *Neuron*, 113(16), 2582-2598.

Hinton, G.E., McClelland, J.L., & Rumelhart, D.E. (1986). Distributed representations. In D.E. Rumelhart, J.L. McClelland, & the PDP Research Group (Eds.), *Parallel Distributed Processing*, Vol. 1. MIT Press.

Hornik, K., Stinchcombe, M., & White, H. (1989). Multilayer feedforward networks are universal approximators. *Neural Networks*, 2(5), 359-366.

Huxley, T.H. (1874). On the hypothesis that animals are automata, and its history. *The Fortnightly Review*, 16(95), 555-580.

IIT-Concerned, Klincewicz, M., Cheng, T., et al. (2025). What makes a theory of consciousness unscientific? *Nature Neuroscience*, 28, 689-693.

Jackson, F. (1982). Epiphenomenal qualia. *Philosophical Quarterly*, 32(127), 127-136.

James, W. (1890). *The Principles of Psychology*. Henry Holt and Company.

Kim, J. (1993). The non-reductivist's troubles with mental causation. In J. Heil & A. Mele (Eds.), *Mental Causation*. Oxford University Press.

Kirkeby-Hinrup, A., Fink, S.B., & Overgaard, M. (2025). The Multiple Generator Hypothesis. *Neuroscience of Consciousness*, 2025(1), niaf035.

Klüver, H. (1966). *Mescal and Mechanisms of Hallucinations*. University of Chicago Press.

Kuhn, T.S. (1962). *The Structure of Scientific Revolutions*. University of Chicago Press.

LaBerge, S. (1985). *Lucid Dreaming*. Ballantine Books.

Lamme, V.A.F. (2006). Towards a true neural stance on consciousness. *Trends in Cognitive Sciences*, 10(11), 494-501.

Lamme, V.A.F. (2010). How neuroscience will change our view on consciousness. *Cognitive Neuroscience*, 1(3), 204-220.

Lau, H. & Rosenthal, D. (2011). Empirical support for higher-order theories of conscious awareness. *Trends in Cognitive Sciences*, 15(8), 365-373.

Levine, J. (1983). Materialism and qualia: The explanatory gap. *Pacific Philosophical Quarterly*, 64(4), 354-361.

Libet, B. (1985). Unconscious cerebral initiative and the role of conscious will in voluntary action. *Behavioral and Brain Sciences*, 8(4), 529-539.

Llinás, R.R. & Ribary, U. (1993). Coherent 40-Hz oscillation characterizes dream state in humans. *Proceedings of the National Academy of Sciences*, 90(5), 2078-2081.

Llinás, R.R., Ribary, U., Contreras, D., & Pedroarena, C. (1998). The neuronal basis for consciousness. *Philosophical Transactions of the Royal Society of London B*, 353(1377), 1841-1849.

Long, R., Sebo, J., Butlin, P., Birch, J., Chalmers, D., et al. (2024). Taking AI welfare seriously. *arXiv*:2411.00986.

Melloni, L., et al. (2023). An adversarial collaboration protocol for testing contrasting predictions of global neuronal workspace and integrated information theory. *PLOS ONE*, 18(2), e0268577.

Metzinger, T. (2003). *Being No One: The Self-Model Theory of Subjectivity*. MIT Press.

Metzinger, T. (2009). *The Ego Tunnel: The Science of the Mind and the Myth of the Self*. Basic Books.

Milinkovic, B. & Aru, J. (2025). Biological computationalism. *Neuroscience & Biobehavioral Reviews*, 181, 106524.

Monti, M.M., et al. (2010). Willful modulation of brain activity in disorders of consciousness. *New England Journal of Medicine*, 362(7), 579-589.

Nagel, T. (1974). What is it like to be a bat? *Philosophical Review*, 83(4), 435-450.

Owen, A.M., et al. (2006). Detecting awareness in the vegetative state. *Science*, 313(5792), 1402.

Penrose, R. & Hameroff, S. (1994). Orchestrated reduction of quantum coherence in brain microtubules: A model for consciousness. *Mathematics and Computers in Simulation*, 40(3-4), 453-480.

Pinto, Y., et al. (2017). Split brain: Divided perception but undivided consciousness. *Brain*, 140(5), 1231-1237.

Priesemann, V., et al. (2013). Neuronal avalanches differ from wakefulness to deep sleep — evidence from intracranial depth recordings in humans. *PLOS Computational Biology*, 9(3), e1002985.

Priesemann, V., et al. (2014). Spike avalanches in vivo suggest a driven, slightly subcritical brain state. *Frontiers in Systems Neuroscience*, 8, 108.

Reinders, A.A.T.S., et al. (2003). One brain, two selves. *NeuroImage*, 20(4), 2119-2125.

Reinders, A.A.T.S., et al. (2008). Cross-examining dissociative identity disorder: Neuroimaging and etiology on trial. *Neurocase*, 14(1), 44-53.

Revonsuo, A. (1999). Binding and the phenomenal unity of consciousness. *Consciousness and Cognition*, 8(2), 173-185.

Rodriguez, E., et al. (1999). Perception's shadow: Long-distance synchronization of human brain activity. *Nature*, 397(6718), 430-433.

Rosenthal, D. (2005). *Consciousness and Mind*. Oxford University Press.

Schartner, M., et al. (2017). Increased spontaneous MEG signal diversity for psychoactive doses of ketamine, LSD and psilocybin. *Scientific Reports*, 7, 46421.

Schurger, A., Sitt, J.D., & Dehaene, S. (2012). An accumulator model for spontaneous neural activity prior to self-initiated movement. *Proceedings of the National Academy of Sciences*, 109(42), E2904-E2913.

Schwitzgebel, E. (2025). AI and consciousness. *arXiv*:2510.09858.

Seth, A. (2021). *Being You: A New Science of Consciousness*. Dutton.

Singer, W. & Gray, C.M. (1995). Visual feature integration and the temporal correlation hypothesis. *Annual Review of Neuroscience*, 18, 555-586.

Strawson, G. (2006). Realistic monism: Why physicalism entails panpsychism. *Journal of Consciousness Studies*, 13(10-11), 3-31.

Tagliazucchi, E., et al. (2012). Criticality in large-scale brain fMRI dynamics unveiled by a novel point process analysis. *Frontiers in Physiology*, 3, 15.

Tagliazucchi, E., et al. (2016). Increased global functional connectivity correlates with LSD-induced ego dissolution. *Current Biology*, 26(8), 1043-1050.

Tegmark, M. (2000). Importance of quantum decoherence in brain processes. *Physical Review E*, 61(4), 4194-4206.

Timmermann, C., et al. (2019). Neural correlates of the DMT experience assessed with multivariate EEG. *Scientific Reports*, 9, 16324.

Timmermann, C., et al. (2023). Human brain effects of DMT assessed via EEG-fMRI. *Proceedings of the National Academy of Sciences*, 120(13), e2218949120.

Tononi, G. (2004). An information integration theory of consciousness. *BMC Neuroscience*, 5, 42.

Tononi, G., Albantakis, L., Barbosa, L., et al. (2025). Consciousness or pseudo-consciousness? A clash of two paradigms. *Nature Neuroscience*, 28, 694-702.

Treisman, A. (1996). The binding problem. *Current Opinion in Neurobiology*, 6(2), 171-178.

Treisman, A. & Gelade, G. (1980). A feature-integration theory of attention. *Cognitive Psychology*, 12(1), 97-136.

Van Rullen, R. & Koch, C. (2003). Is perception discrete or continuous? *Trends in Cognitive Sciences*, 7(5), 207-213.

von Neumann, J. (1932). *Mathematische Grundlagen der Quantenmechanik*. Springer.

Wegner, D.M. (2002). *The Illusion of Conscious Will*. MIT Press.

Weiskrantz, L. (1986). *Blindsight: A Case Study and Implications*. Oxford University Press.

Wigner, E. (1961). Remarks on the mind-body question. In I.J. Good (Ed.), *The Scientist Speculates*. Heinemann.

Wolfram, S. (2002). *A New Kind of Science*. Wolfram Media.

Zurek, W.H. (2003). Decoherence, einselection, and the quantum origins of the classical. *Reviews of Modern Physics*, 75(3), 715-775.
