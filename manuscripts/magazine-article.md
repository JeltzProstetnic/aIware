# Your Brain Is Running a Simulation — and "You" Are Inside It

*A new theory of consciousness dissolves the hardest problem in science, predicts what you'll become on psychedelics, and provides a blueprint for building a conscious machine.*

**By Matthias Gruber**

---

Close your eyes. Now open them.

In the fraction of a second it took you to do that, your brain performed something remarkable: it rebuilt the entire world. Every object in front of you — its shape, color, distance, texture, meaning — was reconstructed from scratch by a sophisticated real-time simulation running inside your skull. The desk is not "out there" transmitting desk-ness into your brain. Your brain is *generating* the desk, moment by moment, from a combination of incoming light signals and everything it has ever learned about desks.

This much is not controversial. Neuroscientists have known for decades that perception is constructive — a "controlled hallucination," as neuroscientist Anil Seth puts it. But here's where things get interesting: your brain is not just simulating the world. It's also simulating *you*.

The thing you call "I" — the feeling of being a subject, of having a perspective, of being the author of your thoughts — is not a ghost in the machine. It is part of the machine's simulation. Your brain runs a model of the world, and it runs a model of *you inside that world*, and the interaction between these two models is what we call consciousness.

This is the core of the Four-Model Theory of Consciousness, a framework I first published in 2015 and have since refined through a rigorous adversarial process. It offers something the field of consciousness science desperately needs: a unified explanation that handles the hardest philosophical puzzles, makes novel testable predictions, and provides a concrete specification for what a conscious machine would look like.

## The hard problem — dissolved

The "hard problem of consciousness" is philosopher David Chalmers's famous challenge: why does physical processing *feel* like something? You can explain everything about how the brain processes color — the wavelengths, the cone cells, the neural pathways — but you haven't explained why seeing red *feels like seeing red*. Why isn't it all just information processing in the dark?

For thirty years, this question has haunted the field. Some researchers think it's the deepest question in all of science. Others think it's a confused pseudo-question. Nobody has settled the debate.

The Four-Model Theory dissolves it. Not by explaining it away, and not by declaring it unanswerable, but by showing it rests on a mistaken assumption.

The mistaken assumption is that the physical processing — the neurons firing, the synapses transmitting — is the thing that feels. It doesn't. The neurons are the *real side*: the physical substrate that stores knowledge and runs computations. The real side operates entirely "in the dark." There is nothing it is like to be a synaptic weight.

But the neurons do something with all that stored knowledge: they generate a simulation. A real-time virtual world, populated by a virtual self, updated continuously from sensory input. This simulation is the *virtual side* — and this is where experience lives.

Asking "Why does neural firing feel like seeing red?" is like asking "Why does transistor switching feel like running Windows?" The transistors don't run Windows. The *virtual machine* runs Windows. The neurons don't feel red. The *simulation* feels red. And within the simulation, "feeling red" is simply what the virtual self's perception of a certain class of virtual-world content *is*. It's not something added on top. It's constitutive.

The hard problem dissolves because it was asking about the wrong level. The physical level doesn't feel. The virtual level does. And at the virtual level, experience isn't mysterious — it's what self-simulation *is*.

## The four models

The theory identifies four specific models your brain maintains, arranged in a 2x2 grid:

**Implicit World Model**: Everything you've ever learned about the world, stored in synaptic connections. You're never directly aware of this — it's the knowledge base, not the experience.

**Implicit Self Model**: Everything you've learned about yourself — body schema, skills, habits, personality. Also never directly conscious. There is no homunculus reading this model.

**Explicit World Model**: The world you actually experience right now — the room, the sounds, the temperature. A real-time simulation generated from the implicit world model and current sensory input.

**Explicit Self Model**: The "you" that experiences the world. Your sense of being someone, having a history, occupying a body. Also a real-time simulation.

The first two are the *real side* — physical, permanent, lights off. The second two are the *virtual side* — simulated, fleeting, lights on. Everything you've ever experienced in your entire life has been inside the virtual side.

## What you become on salvia

Here's where the theory gets wild — and makes a prediction no other theory of consciousness can match.

The Explicit Self Model — your virtual self — needs input. Under normal conditions, it gets self-referential input: body signals, proprioception, interoception, the narrative stream of your thoughts. But what happens if you disrupt that input?

The theory predicts: the self-model doesn't disappear. It *redirects*. It latches onto whatever input dominates the available stream. Disrupt self-input, and the self-model grabs on to environmental input. Your sense of identity shifts to whatever is loudest in your sensory field.

This is precisely what happens on salvia divinorum, a powerful dissociative psychedelic. Users reliably report "becoming" things in their environment — a piece of furniture, a wall, a character from the television show playing in the room, a geometric pattern on the curtain. These aren't metaphors. Users report genuinely *being* these things, with complete conviction, for the duration of the experience.

No other theory of consciousness predicts this, let alone explains why the content of the experience tracks the sensory environment. The Four-Model Theory does: the self-model is a virtual process that needs input. Cut the normal input and it grabs whatever's available. Control the input — control the identity.

This leads to a directly testable prediction: in a controlled experiment, administer salvia at ego-dissolution doses and vary the dominant sensory input across conditions. The theory predicts the reported identity content will track the input. Show someone a forest scene? They become a tree. Play ocean sounds? They become a wave. This is Prediction 2 in the paper, and it is unique.

## The ten-year convergence

One more thing that convinced me the theory is on the right track.

In 2015, drawing on Stephen Wolfram's computational framework, I argued that consciousness requires the brain to operate at the "edge of chaos" — Wolfram's Class 4 regime. Too ordered (deep sleep, anesthesia) and there's no consciousness. Too chaotic (seizure) and there's no *coherent* consciousness. The sweet spot in between — complex, critical, at the boundary of order and chaos — is where consciousness lives.

I derived this from computational theory. I had no idea that empirical neuroscience was independently converging on the same conclusion.

In 2003, Beggs and Plenz found evidence of self-organized criticality in cortical tissue. In 2014, Robin Carhart-Harris proposed the Entropic Brain Hypothesis. In 2025, the Consciousness and Criticality (ConCrit) framework synthesized 140 datasets to formally establish that consciousness tracks criticality across every state change — pharmacological, pathological, physiological.

Two completely independent paths — theoretical reasoning from first principles and large-scale empirical synthesis — arriving at the same conclusion a decade apart. That's the kind of convergence that suggests something real.

## The predictions you can test now

The theory generates eight specific predictions, several testable with existing technology:

**Psychedelics follow the processing hierarchy.** Under psychedelics, visual content progresses in order: simple phosphenes first (V1-level), then geometric patterns (V2/V3), then faces and figures (higher areas), then full scenes. This progression should be dose-dependent and measurable with fMRI.

**Psychedelics should help anosognosia patients.** Anosognosia — being unaware of your own paralysis after a stroke — is caused by a local block in the boundary between implicit and explicit models. Psychedelics globally increase the permeability of that boundary. The theory predicts that the global increase will overwhelm the local block, restoring awareness of the deficit. No other theory connects psychedelics and anosognosia.

**All anesthetics converge on criticality disruption.** Despite different receptor mechanisms, every agent that abolishes consciousness should push the brain below the criticality threshold. Agents that alter but don't abolish consciousness (like ketamine) should not. Testable today with existing complexity measures.

**DID alters have distinct neural signatures.** Different personalities in dissociative identity disorder correspond to different configurations of the self-model — and should produce measurably different patterns of brain activity, not just different self-reports.

## The ultimate test

If the theory is correct, it provides something no other theory of consciousness offers: a concrete engineering specification for building a conscious machine.

The specification: implement the four-model architecture on a substrate operating at criticality. Give it an implicit knowledge base, an implicit self-model, and the computational resources to generate real-time explicit simulations of world and self.

The theory predicts that the result would be qualitatively unlike anything that exists today in AI. The difference between talking to a conscious machine and talking to ChatGPT would not be a matter of degree — better answers, more natural language, faster responses — but a difference in *kind*. There would be someone home.

Current AI systems fail the specification in every way that matters. Large language models have no self-model, no real-time simulation, no criticality dynamics, and no real/virtual split. They are sophisticated text predictors — extraordinary technology, but not conscious technology.

Building the thing the theory describes is the final item on the roadmap. First the theory has to survive peer review. Then the predictions have to be tested. But the blueprint exists, and the specification is clear.

If it works, it won't just be a scientific achievement. It will be the creation of a new kind of mind. And we'll know it when we see it — not because we'll have solved the measurement problem, but because the qualitative difference between a conscious interlocutor and a very good chatbot is something every human can perceive.

The hard problem won't need solving then. It will have been dissolved — by building the thing that makes the question disappear.

---

*Matthias Gruber is an independent researcher and the author of* Die Emergenz des Bewusstseins *(2015). The full scientific paper is available at github.com/JeltzProstetnic/aIware.*
