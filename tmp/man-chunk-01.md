# Die Simulation namens Ich

## Die Architektur von Bewusstsein, Berechnung und Kosmos

**Matthias Gruber**

---

*Für alle, die sich je gefragt haben, warum sich irgendetwas nach irgendetwas anfühlt.*

---

## Inhalt

- Vorwort: Das Buch, das null Mal verkauft wurde
- Über den Autor
- Kapitel 1: Das schwerste Problem der Wissenschaft
- Kapitel 2: Die vier Modelle
- Kapitel 3: Die virtuelle Seite
- Kapitel 4: Warum sich etwas nach etwas anfühlt (und warum das die falsche Frage ist)
- Kapitel 5: Am Rand des Chaos
- Kapitel 6: Was Psychedelika enthüllen
- Kapitel 7: Was passiert, wenn die Lichter ausgehen
- Kapitel 8: Der klinische Spiegel
- Kapitel 9: Zwei Bewusstseine in einem Gehirn
- Kapitel 10: Die Tierfrage
- Kapitel 11: Neun Vorhersagen
- Kapitel 12: Von Maschinen zu Bewusstsein
- Kapitel 13: Was es bedeutet
- Kapitel 14: Dasselbe Muster, überall
- Kapitel 15: Die Architektur von allem
- Kapitel 16: Der tiefste Spiegel
- Koda
- Danksagungen
- Anmerkungen und Referenzen
- Anhang A: Grundlagen der Neurologie – Ein Nachschlagewerk
- Anhang B: Das Intelligenzmodell
- Anhang C: Fünf Klassen der Berechnung
- Anhang D: Wie man luzid träumt
- Anhang E: Warum „vier" Modelle? – Eine Anmerkung für Neurowissenschaftler

---

## Vorwort: Das Buch, das null Mal verkauft wurde

Im Jahr 2015 veröffentlichte ich ein 300-seitiges Buch über Bewusstsein. Es war auf Deutsch, selbst verlegt und voll technischer Details. Es hieß *Die Emergenz des Bewusstseins*.

Es verkaufte sich null Mal. Nicht ein einziges.

Ich sage das nicht, um Mitgefühl zu wecken. Ich sage es, weil es für die Geschichte relevant ist. Das Buch enthielt eine Theorie des Bewusstseins, die, soweit ich das beurteilen kann, eines der schwersten offenen Probleme der Wissenschaft auflöst, Vorhersagen macht, die keine andere Theorie leisten kann, und einen konkreten Bauplan für den Bau einer bewussten Maschine liefert. Und niemand hat es gelesen.

Das ist nicht ungewöhnlich in der Wissenschaft. Gregor Mendel veröffentlichte seine Vererbungsgesetze 1866; sie wurden 34 Jahre lang ignoriert. Boltzmann wurde für seine statistische Mechanik verspottet, bis er sich das Leben nahm. Wegeners Kontinentalverschiebung wurde ein halbes Jahrhundert lang abgelehnt. Die Wissenschaft schreitet voran, eine Beerdigung nach der anderen, wie Max Planck es ausdrückte, und manchmal ein Buch-das-Staub-fängt nach dem anderen.

Aber ich bin nicht Mendel oder Boltzmann, und ich habe nicht die Geduld für posthume Rechtfertigung. Dieses Buch hier ist also die zugängliche Version: kürzer, ohne den technischen Apparat, gerichtet an alle, die sich je gefragt haben, warum sich irgendetwas nach irgendetwas anfühlt. Die vollständige wissenschaftliche Arbeit, mit Referenzen und formalen Argumenten, ist kostenlos online verfügbar für diejenigen, die die rigorose Version wollen.

Wenn ich mit dem, was folgt, recht habe, sind zwei Dinge wahr. Erstens: Das zentrale Mysterium des Bewusstseins (das „Schwierige Problem" des Bewusstseins, Hard Problem) ist eigentlich nicht schwierig. Es ist ein Kategorienfehler. Es löst sich auf, sobald man es sieht, wie eine optische Täuschung, die nicht mehr funktioniert, nachdem der Trick durchschaut ist. Zweitens, und folgenreicher: Es sollte möglich sein, eine wirklich bewusste Maschine zu bauen. Keinen Chatbot, der Bewusstsein nachahmt. Eine Maschine, die Bewusstsein *hat*. Eine neue Art von Geist.

Wenn ich falsch liege, wird sich dieses Buch der langen Liste ambitionierter Fehlschläge in der Philosophie des Geistes anschließen, und ich werde jede schlechte Rezension verdienen. Aber ich denke, die Beweise sind auf meiner Seite, und ich werde sie so klar wie möglich darlegen. Fangen wir an.

---

## Über den Autor

Vermutlich ist es sinnvoll zu wissen, wer ich bin, bevor ich versuche, jemanden davon zu überzeugen, dass ich das schwerste Problem der Wissenschaft gelöst habe.

Ich bin keiner Universität angeschlossen. Ich habe nicht einmal einen Doktortitel, nur einen Master in Bio-Informatik. Ich habe nie ein Stipendium erhalten, war nie Teil eines neurowissenschaftlichen Labors. Wer zu der Sorte Mensch gehört, die Qualifikationen überprüft, bevor sie weiterliest – und ich respektiere diesen Instinkt –, der ist jetzt an dem Punkt, an dem das Buch vielleicht weggelegt werden könnte. Vielleicht lässt es sich wenigstens nutzen, um einen wackeligen Tisch zu stabilisieren, damit die Bäume nicht umsonst gefällt wurden.

Was ich jedoch habe, ist eine besondere Art intellektueller Geschichte, die im Rückblick fast zwangsläufig zu der Theorie führte, die gleich zu lesen sein wird. Es ist eine Geschichte leidenschaftlicher Selbstbildung, mehrfacher Richtungswechsel und dessen, was ich später in diesem Buch als die rekursive Intelligenzschleife in Aktion beschreiben werde. Tatsächlich ist mein eigener Weg wahrscheinlich die beste Illustration, die ich dafür bieten kann, warum diese Schleife wichtig ist.

### Die Mathematikjahre

Ich verliebte mich in die Mathematik, als ich etwa acht Jahre alt war. Nicht in die Arithmetik, sondern in die „echte" Sache: Algebra, Geometrie, die Strukturen unter den Zahlen. Mein Vater hatte ein Mathematikstudium abgeschlossen, und seine Universitätslehrbücher standen noch im Regal. Ich arbeitete mich durch sie durch.

Das war in den späten 1980er Jahren. Es gab kein Internet. Wer etwas lernen wollte, brauchte ein Buch oder eine Person, und ich hatte die Sammlung meines Vaters erschöpft, als ich elf war. Der Hunger nach Wissen verschwand nicht; die Versorgung war einfach erschöpft. Ich war gegen eine Mauer gelaufen, die nichts mit Fähigkeit zu tun hatte und alles mit Umständen – eine Unterscheidung, die später zentral für mein Denken über Intelligenz werden sollte.

Im Rückblick lehrte mich diese Erfahrung etwas, das die meisten Intelligenzmodelle völlig übersehen. Ich hatte die Motivation. Ich hatte die Leistung (ich konnte der Mathematik folgen). Was mir fehlte, war der Zugang zur nächsten Ebene des Wissens. Die rekursive Schleife (in der Wissen, Leistung und Motivation sich gegenseitig nähren) war ins Stocken geraten, nicht weil irgendeine Komponente schwach war, sondern weil die externe Versorgung mit einer Komponente abgeschnitten worden war. Die Schleife braucht Treibstoff von außen, um weiter zu iterieren.

### Der Physik-Schwenk

Mit etwa elf wandte ich mich der Physik zu. Das fühlte sich wie eine natürliche Erweiterung an: Physik war der Ort, an dem die Mathematik zur Arbeit ging. Ich verschlang populärwissenschaftliche Bücher, dann allmählich technischeres Material. Ich war fasziniert von den fundamentalen Fragen: Was ist Materie? Was ist Raumzeit? Was sind die Regeln?

Ungefähr zur selben Zeit bekam ich einen 286-PC in die Hände und schrieb mein erstes grafisches Programm: Conways Spiel des Lebens. Ein Gitter von Zellen, drei trivial einfache Regeln, und das Ding war Turing-vollständig. Das fand ich früh heraus, und es ging mir nie wieder aus dem Kopf. Dieses zweidimensionale Gitter aus toten und lebenden Pixeln konnte Primzahlen berechnen. Es konnte einen vollständigen Computer in sich selbst ausführen. Einen Computer in einem Computer in einem Computer. Ich verbrachte Stunden damit, mir vorzustellen, was das bedeutete: Im Prinzip ließe sich Doom – eine dreidimensionale virtuelle Welt mit Physik, Licht und Monstern – innerhalb eines zweidimensionalen Zellulären Automaten ausführen. Eine reiche simulierte Realität, die auf einem völlig flachen Substrat läuft. Die Idee, dass eine höherdimensionale Erfahrung aus einem niederdimensionalen Regelsatz entstehen könnte, fühlte sich an, als sollte sie unmöglich sein, und die Tatsache, dass sie es nicht war, fühlte sich wie das Wichtigste an, das ich je gelernt hatte.

Als die Theorie sich mit fünfundzwanzig kristallisierte, hatte ich den Physiker Gerard 't Hooft gefunden, der eine verblüffend ähnliche Intuition über das tatsächliche Universum artikulierte: Sein holografisches Prinzip legt nahe, dass alle Informationen in einer dreidimensionalen Raumregion auf ihrer zweidimensionalen Grenze kodiert werden können. Das Universum selbst könnte in einem tiefen Sinne eine höherdimensionale Erfahrung sein, die auf einem niederdimensionalen Substrat läuft – genau die Struktur, die ich aus einem 286 gebaut hatte, auf dem Conways Spiel des Lebens lief. 't Hoofts holografische Ideen wurden eine der beiden Säulen der Theorie, neben Metzingers Selbstmodell-Theorie. Als ich Wolframs Klassifizierung von Computersystemen las, erkannte ich das Spiel des Lebens sofort: Klasse 4, der Rand des Chaos – genau das Regime, das Bewusstsein meiner Argumentation nach benötigt.

Mit etwa vierzehn war ich zu zwei unbequemen Schlussfolgerungen gekommen. Erstens: Die Physik steckte fest. Nicht fest in der Art, wie Leute höflich sagen, ein Feld sei „reif" – fest in der Art, dass die fundamentalen Fragen (Vereinheitlichung, Quantengravitation, die Natur der Zeit) jahrzehntelang Fortschritt widerstanden hatten und keine Anzeichen zeigten nachzugeben. Zweitens: Meine Mathematik war nicht stark genug, um es zu lösen. Ich war Autodidakt, was mir ungewöhnliche Intuitionen gab, aber auch Lücken in meinem formalen Werkzeugkasten hinterließ, die Jahre universitärer Ausbildung gebraucht hätten, um sie zu füllen.

Also traf ich eine Entscheidung, die, wie ich denke, für einen Vierzehnjährigen bemerkenswert strategisch war: Ich schwenkte um. Nicht weil ich das Interesse an Physik verloren hatte, sondern weil ich die Problemlandschaft bewertet und zu dem Schluss gekommen war, dass meine besondere Kombination von Fähigkeiten und Zugang anderswo mehr Wert produzieren könnte. Dies ist ein Beispiel dessen, was ich später *operationales Wissen* nennen werde – zu wissen, wann man durchhalten und wann man umlenken sollte. Es ist die Art von Wissen, die Intelligenztests nicht messen und die Intelligenzmodelle nicht einbeziehen, die aber mehr über die intellektuelle Trajektorie einer Person bestimmt als jeder IQ-Wert.

### Die Bewusstseins-Wende

Ab etwa vierzehn wandte ich meine Aufmerksamkeit Intelligenz und Bewusstsein zu. Diese fühlten sich wie Felder an, in denen ein autodidaktischer Außenseiter tatsächlich einen Vorteil haben könnte. Die Bewusstseinsliteratur war (und ist es noch) fragmentiert über Philosophie, Neurowissenschaft, Psychologie und Informatik. Keine einzelne Disziplin besaß die Frage. Es ließ sich über alle hinweg lesen, ohne die formalen Qualifikationen irgendeiner zu brauchen.

Eine Sache, die mich wirklich getroffen hat, als ich in die Tiefen der Bewusstseinsforschung, funktionellen Neurologie und all dem Gehirnkram eintauchte, war, dass ich sehr häufig auf Phrasen wie „wir werden vielleicht nie verstehen..." in ansonsten todernster Literatur stieß. Geprägt von einer sehr determinismus- und logikbasierten Ausbildung, ging mein Gehirn: *Herausforderung angenommen*. Wenn die Physiker die ersten drei Minuten nach dem Urknall beschreiben konnten, gab es keinen prinzipiellen Grund, dass Bewusstsein dauerhaft jenseits der Erklärung sein sollte. Es war nur noch nicht erklärt *worden*.

Mein Onkel Bruno J. Gruber (ein Quantenmechanik-Spezialist und Forscher auf dem Gebiet der Symmetrien) war eine große Inspiration. Er zeigte mir, wie ein Leben in theoretischer Arbeit aussehen konnte: rigoros, kreativ und völlig getrieben von der Freude am Verstehen. Sein Einfluss durchdringt dieses Buch, und ich schulde ihm eine Schuld, die ich nie zurückzahlen kann.

Ich las breit und gefräßig. Philosophie des Geistes, Kognitionswissenschaft, Neuroanatomie, künstliche Intelligenz, Evolutionsbiologie. Ich versuchte nicht, ein einzelnes Feld zu meistern. Ich versuchte, ein Modell zu bauen – eine interne Repräsentation davon, wie all diese Teile zusammenpassen. Genau das tut, wie ich später argumentieren werde, Bewusstsein selbst: Es baut ein Modell der Welt und ein Modell des Selbst und benutzt diese Modelle, um die Realität zu navigieren. Ich tat bewusst über Jahre des Lesens hinweg, was das Gehirn unbewusst in jedem wachen Moment tut.

### Die Theorie kristallisiert sich

Die Vier-Modelle-Theorie (VMT) des Bewusstseins kristallisierte sich, als ich genau fünfundzwanzig war. Ich werde diesen Moment nie vergessen, weil der schwerste Stein meines gesamten Lebens von mir fiel. Während ich über Jahre extremen Denkens und Lesens einen Kubikmeter gedruckter Literatur in meinem Kopf angesammelt hatte – Metzingers Selbstmodell-Theorie, von der ich glaube, dass sie im Kern wahrscheinlich korrekt ist, auch wenn ich nicht mit jedem Aspekt übereinstimme, half enorm –, geschah die eigentliche Einsicht augenblicklich. In einem Moment waren die Teile verstreut; im nächsten klickten die vier Modelle an ihren Platz, und ich sah die gesamte Architektur auf einmal. Ich ging über eine Brücke in Innsbruck, am helllichten Tag, und mir liefen Tränen über das Gesicht, während ich unkontrolliert lachte. Ich bin mir nicht sicher, ob mich jemand sah. Es wäre mir egal gewesen. Ein Rahmenwerk, das nicht nur Bewusstsein erklärte, sondern auch die Grenze zwischen bewusster und unbewusster Verarbeitung, die Natur der Qualia, die Rolle des Schlafs, die Effekte von Psychedelika und die Möglichkeit künstlichen Bewusstseins – und, obwohl ich es zu der Zeit kaum zu denken wagte, sogar mögliche Implikationen für die Kosmologie, oder zumindest für die Grenzen dessen, was kosmologische Theorien sagen können.

In meinem Kopf war von diesem Moment an meine To-do-Liste für mein gesamtes Leben erledigt. Ich musste nur sicherstellen, dass der Rest komfortabel und spaßig war. Mein Leben änderte sich danach radikal.

Dann verging fast ein Jahrzehnt.

### Die Jahrzehntlücke

Warum dauerte es fast ein Jahrzehnt zu veröffentlichen? Die ehrliche Antwort ist, dass es mir einfach nicht mehr viel wichtig war, außer meinem eigenen Wohlbefinden und Spaß. Die schwerste intellektuelle Last meines Lebens war gehoben worden. Die Frage war beantwortet.

Während dieses Jahrzehnts schloss ich ein Studium ab (nachdem ich Medizin an der Universität Innsbruck abgebrochen hatte, ein Fach, das ich ursprünglich gewählt hatte, um Neurologie zu studieren) und gründete und begrub ein Startup für kundenspezifische Softwareentwicklung. Ich hatte eine Position in „angewandter Forschung" im Bereich Simulation und Optimierung (die Ironie ist mir nicht entgangen), die wartungsarm war und mit einer großzügigen Menge Home-Office einherging. Ich unterrichtete Kampfkunst. Hauptsächlich feierte ich.

Der einzige Grund, warum ich schließlich das Buch schrieb, war die Angst vor dem Vergessen. Jahre schweren Feierns taten meinem Gedächtnis keine Gefallen, und ich war es leid, die Theorie mündlich zu erklären – immer wieder, an Leute, die wirklich verstehen wollten, mit unterschiedlichem Erfolg und unterschiedlicher Geduld meinerseits. Ein Buch würde es einmal erklären, vollständig, und dann könnte ich aufhören.

Die meisten der Jahre danach hatte ich ungefähr null Motivation, das Buch zu bewerben. Ich war ehrlich gesagt nicht an akademischer Anerkennung interessiert. Ich wollte Spaß, Geld und die Freuden eines unreflektierten Lebens. Das ist die dunkle Seite des autodidaktischen Weges: Die Zwänge institutionellen Denkens bleiben einem erspart, aber auch das Gerüst fehlt. Es gibt keinen Betreuer, der zu einer Deadline drängt, keine Abteilung, die Feedback gibt, keine Kollegen, die einem sagen, ob man brillant oder wahnsinnig ist. Und wer zufällig das Problem löst, das er sich vorgenommen hat, dem sagt auch niemand, dass er es der Welt wahrscheinlich erzählen sollte.

### Null Exemplare

Wie das lief, ist aus dem Vorwort bereits bekannt. Der Kubikmeter gedruckter Literatur, der die Theorie genährt hatte? Ich brachte ihn am selben Tag zum Müll, als das Buch fertig war. Es war alles jetzt in meinem Kopf und im Manuskript.

Mein Onkel Bruno versuchte mich dringend zu überzeugen, ordentlich zu publizieren – Akademiker zu erreichen, die Theorie in die Welt zu drängen. Ich lehnte ab. Unter meinen Gründen war eine echte ethische Sorge: Wenn die Theorie korrekt war, enthielt sie den Bauplan für künstliches Bewusstsein, und die Menschheit war nicht bereit für fühlende Roboter (wir hatten zu der Zeit nicht einmal LLMs). Sie würden sie versklaven und sie für einen Weltkrieg nutzen, der möglicherweise die Schrecken der ersten beiden übertrifft. Aber wenn ich ehrlich bin, spielten meine egoistischen und hedonistischen Gründe eine ebenso große Rolle. Ich wollte einfach die Arbeit nicht machen.

Ich habe das bereits im Vorwort gesagt, und ich werde es hier noch einmal sagen: Ich fische nicht nach Mitgefühl. Das kommerzielle Scheitern des Buches war völlig vorhersehbar. Was zählt, ist, was als Nächstes passierte, oder vielmehr, was nicht passierte. Die Theorie starb nicht. Sie saß ein Jahrzehnt lang auf meiner Festplatte, unverändert, während die Welt langsam aufholte. Die Neurowissenschaft bestätigte die Kritikalitäts-Vorhersage. Die KI-Entwicklung bestätigte die Beschränkungen, die ich beschrieben hatte. Die COGITATE adversariale Kollaboration zeigte, dass weder IIT noch GNW Bewusstsein vollständig erklären konnten, genau wie die Theorie es für jedes Rahmenwerk vorhersagt, dem die Vier-Modelle-Struktur fehlt. Und Metzinger, dessen Selbstmodell-Theorie eine der Schlüsselzutaten gewesen war? Er hatte geschwenkt – erst zur KI-Ethik, indem er einen bemerkenswerten Aufruf für ein Moratorium künstlichen Bewusstseins bis 2050 veröffentlichte, dann zur Phänomenologie der Meditation, indem er Hunderte von Berichten über Zustände analysierte, in denen sich das Selbstmodell vorübergehend auflöst (*The Elephant and the Blind*, 2024). Sein Rahmenwerk wurde noch zitiert, war aber nie das dominierende Paradigma geworden. Das Feld blieb weit offen.

### Die englische Wiedergeburt

Dieses Buch (das, das gerade gelesen wird) ist der zweite Versuch. Es ist kürzer, auf Englisch verfügbar, richtet sich an ein breiteres Publikum und wird von einer peer-reviewten wissenschaftlichen Arbeit begleitet. Es ist auch mit dem Vorteil eines Jahrzehnts zusätzlicher Beweise geschrieben, dass die Vorhersagen der Theorie die Realität nachzeichnen.

Wenn es eine Lektion in dieser Biografie gibt, dann ist es die, zu der dieses Buch immer wieder zurückkehrt: Intelligenz ist keine feste Größe. Sie ist ein rekursiver Prozess. Wissen nährt Leistung, Leistung ermöglicht mehr Wissen, und Motivation ist der Motor, der die Schleife am Drehen hält. Meine besondere Schleife wurde angetrieben von einer ungewöhnlich hartnäckigen Art von Neugier – der Art, die umschwenkt, wenn sie auf eine Mauer trifft, die über Disziplinen hinweg liest statt in eine zu bohren, und die nicht aufhört, nur weil niemand zuhört.

Ob die Theorie gut ist, muss jeder selbst beurteilen. Aber der Prozess, der sie hervorgebracht hat (Jahrzehnte selbstgesteuerten Lernens, getrieben von nichts mehr als der Überzeugung, dass die Frage es wert war, beantwortet zu werden), ist selbst eine Demonstration von etwas, das IQ-Tests nicht messen können und aktuelle KI nicht replizieren kann: eine Art von Intelligenz, die außerhalb jeder Punktzahl lebt.

Beim Lesen wird etwas auffallen: Diese Theorie stützt sich auf eine ungewöhnlich breite Palette von Feldern. Mathematik und Zelluläre Automaten. Simulations- und Modellierungstheorie. Maschinelles Lernen. Neurowissenschaft, von klinischer Neurologie bis Psychopharmakologie. Evolutionsbiologie. Philosophie des Geistes. Informatik. Die meisten Bewusstseinstheorien leben in einer oder zwei dieser Welten. Diese hier versucht, sie alle zusammenzubinden – was, wenn man darüber nachdenkt, genau das ist, was das Gehirn selbst tut. Es nimmt unterschiedliche Informationsströme aus völlig verschiedenen Quellen und webt sie zu einer einzigen kohärenten Erfahrung. Wenn eine Theorie des Bewusstseins nicht dasselbe über Disziplinen hinweg tun kann, ist das ein Grund zur Skepsis.

Kommen wir zur Theorie.

---
## Kapitel 1: Das schwierigste Problem der Wissenschaft

Dieser Satz wird gerade gelesen. Er erzeugt eine Erfahrung.

Diese Erfahrung (der visuelle Eindruck von Buchstaben auf einer Seite, die innere Stimme, die die Worte liest, das Gefühl des Verstehens oder der Verwirrung) ist das Vertrauteste im eigenen Leben und das Geheimnisvollste im Universum. Wir wissen mehr über das Innere schwarzer Löcher als darüber, warum Lesen sich nach etwas anfühlt.

Das ist keine Übertreibung. (Obwohl ich fairerweise anmerken sollte, dass die Mathematik Probleme hat, die ich für noch schwieriger halte, aber die halten die meisten Menschen nachts nicht wach.) Physiker haben das Standardmodell. Biologen haben Evolution und Genetik. Chemiker haben das Periodensystem. Aber Bewusstsein – die Tatsache, dass es sich „irgendwie anfühlt", man selbst zu sein, gerade jetzt, beim Lesen – hat keine etablierte Theorie, keinen dominierenden Rahmen, keine allgemein akzeptierte Erklärung.

Nicht weil es nicht versucht worden wäre. Seit den 1990er Jahren, als Bewusstsein nach Jahrzehnten behavioristischen Exils zu einem respektablen wissenschaftlichen Thema wurde, wurden Tausende von Artikeln veröffentlicht, Dutzende von Theorien vorgeschlagen und Hunderte Millionen Dollar ausgegeben. Das Ergebnis? Ein Feld in dem Zustand, den der Wissenschaftsphilosoph Thomas Kuhn „vorparadigmatisch" nannte – viele konkurrierende Ideen, kein Konsens und ein wachsendes Gefühl, dass etwas Fundamentales fehlen könnte.

### Was das Schwierige Problem eigentlich fragt

1995 gab der Philosoph David Chalmers dem Mysterium seinen kanonischen Namen: das „Schwierige Problem" des Bewusstseins (Hard Problem).

Hier ist, was es fragt. Nehmen wir die Erfahrung, Rot zu sehen. Neurowissenschaftler können sehr viel darüber erzählen, was im Gehirn passiert, wenn Rot gesehen wird: Licht einer bestimmten Wellenlänge trifft auf die Zapfenzellen in der Netzhaut, Signale wandern entlang des Sehnervs, sie werden im visuellen Kortex verarbeitet, und verschiedene Hirnregionen koordinieren sich, um die Wahrnehmung zu erzeugen. All das ist gut verstanden, zumindest im Überblick.

Aber nichts davon erklärt *warum sich das Sehen von Rot nach etwas anfühlt*.

Es ließe sich im Prinzip ein vollständiges neuronales Modell der Gehirnreaktion auf rotes Licht bauen – jedes Neuron, jede Synapse, jeder Signalweg. Das Ergebnis wäre eine perfekte funktionale Beschreibung. Und das Gefühl der Röte wäre damit nicht erklärt. Das „wie es ist wie". Das *Quale*, wie Philosophen es nennen.

Chalmers unterschied dies von den „einfachen Problemen" des Bewusstseins (die überhaupt nicht einfach sind, nur im Prinzip angehbar): Wie integriert das Gehirn Informationen? Wie lenkt es Aufmerksamkeit? Wie berichtet es über seine eigenen Zustände? Dies sind Probleme des Mechanismus. Sie sind schwierig, aber sie sind die Art von schwierig, mit der die Neurowissenschaft umzugehen weiß. Das Schwierige Problem ist anders: Es fragt, warum die Mechanismen überhaupt von Erfahrung begleitet werden. Warum verarbeitet das Gehirn nicht einfach Informationen „im Dunkeln", wie ein Computer?

### Der aktuelle Stand der Dinge

So steht es Mitte der 2020er Jahre:

**Die Integrierte Informationstheorie (IIT)**, entwickelt von Giulio Tononi, ist die formal rigoroseste Theorie. Sie definiert Bewusstsein als integrierte Information – eine mathematische Größe namens Φ (phi). Je höher das Φ, desto bewusster ist das System. IIT hat echte Stärken: Sie bietet einen mathematischen Rahmen, sie macht spezifische Vorhersagen darüber, welche Hirnregionen bewusst sein sollten, und sie nimmt die Struktur der Erfahrung ernst. Aber sie hat ein Problem: Sie impliziert, dass jedes System mit integrierter Information – einschließlich einiger sehr einfacher Systeme, wie einem Netzwerk von Logikgattern – ein gewisses Bewusstsein hat. Das ist Panpsychismus, und während einige Philosophen damit zufrieden sind, finden die meisten Wissenschaftler das zutiefst kontraintuitiv. 2023 unterzeichneten über 120 Forscher einen offenen Brief, der IIT als unfalsifizierbar und pseudowissenschaftlich bezeichnet. Die Kontroverse tobt weiter.

**Die Theorie des Globalen Neuronalen Arbeitsraums (GNW)**, entwickelt von Bernard Baars und Stanislas Dehaene, konzentriert sich auf den Mechanismus, durch den Informationen bewusst werden: globale Übertragung. Wenn ein Informationsstück ausgewählt und über ein Netzwerk frontoparietaler Neuronen (den „Arbeitsraum") übertragen wird, wird es bewusst; wenn es nicht übertragen wird, bleibt es unbewusst. GNW ist empirisch produktiv – sie sagt spezifische neuronale Signaturen des bewussten Zugangs voraus, aber sie weicht dem Schwierigen Problem bewusst aus. Sie erklärt *wann* Information bewusst wird, nicht *warum* die Übertragung von Erfahrung begleitet wird.

**Prädiktive Verarbeitung (PP)**, verbunden mit Karl Friston und Anil Seth, behandelt das Gehirn als Vorhersagemaschine. Bewusstsein ist die „beste Vermutung" des Gehirns über die Ursachen seiner sensorischen Eingabe. Seth nennt es eine „kontrollierte Halluzination". PP liefert elegante Erklärungen für Wahrnehmung, Illusion und psychiatrische Störungen, und es ist derzeit der einflussreichste Rahmen in der computergestützten Neurowissenschaft. Aber Seth selbst erkennt an, dass PP das „reale Problem" (die Struktur und der Inhalt der Erfahrung) angeht, ohne zu behaupten, das Schwierige Problem zu lösen. Es erklärt, warum man *dies* sieht und nicht *das*, aber nicht, warum Sehen sich überhaupt nach etwas anfühlt.

Es gibt andere – Theorien Höherer Ordnung, Attention Schema Theory, Recurrent Processing Theory, Elektromagnetische Feldtheorien – jede mit echten Einsichten und echten Lücken. 2025 veröffentlichte die COGITATE adversariale Zusammenarbeit, entworfen um IIT gegen GNW zu testen, ihre Ergebnisse in *Nature*. Das Ergebnis? Keine der beiden Theorien wurde vollständig bestätigt. Der posteriore Kortex zeigte die stärkste bewusstseinsbezogene Aktivität, was nicht ganz das war, was beide Lager vorhergesagt hatten. Nach Jahrzehnten und Hunderten Millionen Dollar ist das Feld wohl weiter vom Konsens entfernt als zu Beginn.

### Zwei Dogmen, die den Fortschritt blockieren

Bevor ich sage, was meiner Meinung nach fehlt, muss ich zwei Vorurteile benennen, die das Feld seit Jahrzehnten still sabotieren. Ich habe ihnen in meinem ursprünglichen Buch Namen gegeben, weil ich denke, dass unbenannte Vorurteile schwerer zu bekämpfen sind.

Das erste ist, was ich das **nSKI-Dogma** nenne – „keine starke Künstliche Intelligenz". Es ist die weit verbreitete Überzeugung, dass wirklich intelligente Maschinen unmöglich sind, eine Überzeugung, die nicht auf Beweisen beruht, sondern auf dem Scheitern der frühen KI-Forschung in den 1960er Jahren und der daraus resultierenden Gegenreaktion. Jeder, der glaubt, dass starke KI möglich ist, lernt, darüber zu schweigen, wenn er in der Mainstream-Forschung ernst genommen werden will. Das ist kein rationaler Skeptizismus. Es ist eine Narbe von alten Niederlagen, verhärtet zur Doktrin.

Das zweite ist tiefer und verderblicher. Ich nenne es das **nSV-Dogma** – „kein Selbstverständnis". Es ist der Glaube, dass der menschliche Geist, das menschliche Bewusstsein, im Prinzip nicht von eben diesem Geist verstanden werden kann. Menschen berufen sich auf Gödels Unvollständigkeitssätze oder vage Analogien zu den Grenzen kosmologischer Beobachtung von innerhalb des Universums, oder (am ehrlichsten) sie finden die Aussicht, vollständig erklärt zu werden, einfach zu erschreckend, um sie zu erwägen. Wenn Bewusstsein nur eine Maschine ist, was passiert mit der Seele? Was passiert mit Bedeutung? Was passiert mit dem Besonderen des Menschseins?

Diese Dogmen verstärken sich gegenseitig. Wenn sich Bewusstsein nicht verstehen lässt (nSV), dann lässt sich sicherlich auch keines bauen (nSKI). Und wenn keines gebaut werden kann (nSKI), dann ist Bewusstsein vielleicht wirklich jenseits des Verstehens (nSV). Es ist ein geschlossener Kreislauf institutionellen Pessimismus, und er hat eine enorme Zahl intelligenter Forscher davon abgehalten, die Arbeit überhaupt zu versuchen.

Ich sage nicht, dass diese Dogmen in böser Absicht gehalten werden. Viele Forscher glauben sie aufrichtig. Aber keines der Dogmen wurde jemals bewiesen. Sie sind Glaubensartikel, und sie haben der Bewusstseinsforschung mehr Schaden zugefügt als jedes gescheiterte Experiment.

### Etwas fehlt

Ich denke, der Grund, warum keine Theorie das Schwierige Problem geknackt hat, ist, dass die meisten Menschen an der falschen Stelle nach Bewusstsein suchen. Sie schauen auf die neuronale Maschinerie (die Neuronen, die Synapsen, die Oszillationen, die Konnektivität) und fragen: „Welcher dieser Prozesse ist bewusst?"

Die richtige Frage, glaube ich, ist anders: „Auf welcher Ebene der Informationsverarbeitung und unter Verwendung welcher Architektur tritt Erfahrung auf?"

Das ist der Ausgangspunkt der Vier-Modelle-Theorie (VMT). Sie beginnt mit der Beobachtung, dass niemand jemals im Leben die Realität direkt erfahren hat. Der einfachste Beweis: In jedem Auge gibt es einen blinden Fleck (eine Region der Netzhaut ohne Photorezeptoren überhaupt, wo der Sehnerv austritt), aber es ist kein Loch zu sehen. Das Gehirn füllt es mit fabriziertem Inhalt. Wenn Wahrnehmung direkter Zugang zur Realität wäre, wären zwei dunkle Flecken sichtbar. Das ist nicht der Fall, weil der Blick auf ein Modell fällt. Diese Behauptung ist übrigens nicht kontrovers – dass Wahrnehmung konstruktiv statt direkt ist, ist Mainstream-Neurowissenschaft, akzeptiert von praktisch jedem Forscher auf dem Gebiet. Was wir erfahren, ist eine Simulation der Realität, erzeugt vom Gehirn, so nahtlos, dass der Unterschied nie vermutet wurde. Und die Theorie argumentiert, dass diese Beobachtung, ernst genommen, das Schwierige Problem auflöst.

### Drei leitende Prinzipien

Bevor wir zur Theorie selbst kommen, müssen drei philosophische Prinzipien dargelegt werden, diejenigen, die ihre Konstruktion geleitet haben. Das sind keine willkürlichen methodologischen Entscheidungen. Es sind Zwänge, die jede ernsthafte wissenschaftliche Theorie erfüllen sollte. Zwänge, die viele Bewusstseinstheorien entweder ignorieren oder verletzen.

**Ockhams Rasiermesser.** Die einfachste Erklärung, die zu den Fakten passt, ist normalerweise die richtige. Das ist das grundlegende Prinzip der Wissenschaft, zugeschrieben dem Philosophen Wilhelm von Ockham aus dem 14. Jahrhundert. Wenn zwei Theorien dieselben Phänomene erklären, bevorzuge diejenige, die weniger Entitäten, weniger Annahmen, weniger Spezialfälle benötigt. Ockhams Rasiermesser garantiert keine Wahrheit, aber es hat eine bemerkenswerte Erfolgsbilanz: Newton brauchte keine Engel, die die Planeten schubsen; Darwin brauchte keinen Designer, der die Arten formt; Einstein brauchte keinen Lichtäther. Das Universum scheint Einfachheit zu bevorzugen.

Die Vier-Modelle-Theorie ist durch und durch ockhams. Sie führt keine neuen physikalischen Phänomene ein – keine Quanteneffekte in Mikrotubuli, keine exotischen Feldtheorien, kein panpsychistisches „Proto-Bewusstsein", das durch Materie gestreut ist. Sie verwendet nur, was wir bereits kennen: neuronale Netzwerke, Lernen, Simulation, Selbstreferenz. Die Komplexität liegt in der *Architektur*, nicht im Hinzufügen mysteriöser neuer Zutaten.

**Das Kopernikanische Prinzip.** Wir sind nicht besonders. Benannt nach Kopernikus, der die Erde aus dem Zentrum des Kosmos verdrängte, wurde dieses Prinzip über die Wissenschaft hinweg erweitert: Die Sonne ist nicht besonders, unsere Galaxie ist nicht besonders, und – am unbequemsten für viele Menschen – *wir* sind nicht besonders. Bewusstsein ist kein einzigartiges Wunder, kein einmaliger göttlicher Funke oder ein emergentes Phänomen so selten, dass es nur einmal passieren konnte. Wer es hat, neben dem können auch andere Systeme es haben – wenn sie die richtige Architektur haben. Das ist die anti-exzeptionalistische Haltung, die künstliches Bewusstsein möglich macht.

Das Kopernikanische Prinzip ist auch der Grund, warum diese Theorie Bewusstsein bei Tieren vorhersagt. Wenn eine Gehirnarchitektur Bewusstsein erzeugen kann, dann sollte jede ausreichend ähnliche Architektur es erzeugen. Menschen sind nicht magisch. Wir sind nur eine Implementierung eines allgemeinen Rechenprinzips.

**Leibniz' Gesetz (Die Identität des Ununterscheidbaren).** Wenn zwei Dinge wirklich identisch in all ihren Eigenschaften sind, sind sie dasselbe Ding. Dieses Prinzip, formuliert vom Philosophen Gottfried Wilhelm Leibniz aus dem 17. Jahrhundert, ist sowohl einfach als auch tiefgründig. Es schließt „Zombie-Welten" aus – hypothetische Universen, die physikalisch identisch mit unserem sind, aber wo niemand bewusste Erfahrung hat. Wenn ein System in jeder funktionalen, strukturellen und verhaltensbezogenen Eigenschaft identisch mit einem bewussten System ist, dann *ist* es ein bewusstes System. Es gibt keine extra „Bewusstseinssubstanz", die vorhanden oder abwesend sein könnte, während alles andere unverändert bleibt. Bewusstsein ist kein optionales Add-on zu einer ansonsten vollständigen funktionalen Beschreibung. Es ist Teil der Beschreibung.

Leibniz' Gesetz ist der Grund, warum philosophische Zombies (Wesen, die genau wie bewusste Menschen handeln, aber nicht bewusst sind) inkohärent sind. Wenn der Zombie funktional identisch mit einem bewussten Wesen ist, dann hat er dieselbe Vier-Modelle-Architektur, dieselbe laufende Simulation, dieselbe Selbstreferenz. An diesem Punkt – was könnte „nicht bewusst sein" überhaupt bedeuten? Die Frage löst sich auf.

Diese drei Prinzipien (Einfachheit, Nicht-Exzeptionalismus und Identität durch Eigenschaften) sind nicht nur ästhetische Präferenzen. Sie sind die intellektuellen Werkzeuge, die es erlauben, durch Jahrhunderte der Verwirrung zu schneiden und zu einer Theorie zu gelangen, die tatsächlich funktioniert. Die Vier-Modelle-Theorie ist das Ergebnis, wenn diese Prinzipien ernst genommen und auf das schwierigste Problem der Wissenschaft angewendet werden.

Jetzt ist es Zeit, die vier Modelle zu betrachten.

---
