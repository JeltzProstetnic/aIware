**Substrate-Independent Consciousness and the Ethics of Artificial Minds: Implications of the Four-Model Theory**

Matthias Gruber, Independent researcher, ORCID: 0009-0005-9697-1665

---

Can we determine whether an artificial system is conscious — and if so, what ethical obligations follow? This paper argues that both questions become tractable once consciousness is understood as a specific computational process rather than a mysterious property of biological matter.

I present the Four-Model Theory (FMT), a substrate-independent framework that specifies consciousness as real-time self-simulation across two orthogonal axes — scope (world vs. self) and mode (implicit/learned vs. explicit/simulated) — yielding a 2×2 matrix of model kinds. The implicit models constitute accumulated knowledge; the explicit models are transient simulations generated from them in real time. Consciousness arises through self-referential closure: unlike a weather simulation, the four-model architecture creates a closed loop in which the system models its own modeling process. Qualia are virtual — they exist within the simulation, not at the substrate level — dissolving the Hard Problem by revealing a category error in its formulation.

FMT specifies a physical prerequisite: substrate dynamics at or near the edge of chaos (Class 4 computation), independently derived from Wolfram's framework and converging with empirical criticality research. Self-simulation and criticality together provide testable criteria for consciousness in any substrate.

This yields concrete implications. Current LLMs fail FMT's criteria on multiple counts. An AC system would require persistent learned models, real-time self-simulation, self-referential closure, and criticality — deliberate architectural choices, not emergent properties of scale. FMT's graduated consciousness levels (from basic through triply extended) mean ethical obligations scale with simulation depth, offering a middle path between overcautious panpsychism and dismissive functionalism. The ethical question transforms from "might AI be conscious?" to "are we building the architecture that would make it conscious?" — a question amenable to empirical investigation and responsible governance.
