# Your Brain Is Running a Simulation — and "You" Are Inside It

*A unified theory of consciousness explains why anything feels like anything, what happens to your identity on psychedelics, and how to build a conscious machine.*

**By Matthias Gruber**

---

Close your eyes. Now open them.

In the fraction of a second it took you to do that, your brain performed something remarkable: it rebuilt the entire world. Every object in front of you — its shape, color, distance, texture, meaning — was reconstructed from scratch by a real-time simulation running inside your skull. The desk is not "out there" transmitting desk-ness into your brain. Your brain is *generating* the desk, moment by moment, from a combination of incoming light signals and everything it has ever learned about desks.

This much is not controversial. Neuroscientists have known for decades that perception is constructive — a "controlled hallucination," as Anil Seth puts it. But here's where things get interesting: your brain is not just simulating the world. It's also simulating *you*.

The thing you call "I" — the feeling of being a subject, looking out from inside your body, being the author of your thoughts — is not a ghost in the machine. It is part of the machine's simulation. Your brain runs a model of the world, and it runs a model of *you inside that world*, and the interaction between these two models is what we call consciousness.

This is the core of the Four-Model Theory of Consciousness, a framework I published in 2015 and have spent a decade defending against the hardest objections I could find. It offers something the field of consciousness science desperately needs: a unified explanation that handles the deepest philosophical puzzles, makes novel testable predictions, and provides a concrete specification for what a conscious machine would look like.

### Why now?

For decades, consciousness was the untouchable third rail of neuroscience — too philosophical for the scientists, too empirical for the philosophers. But three things have changed. First, AI forced the question: what would make a machine conscious, rather than merely clever? Second, psychedelics returned to the lab after a fifty-year exile, giving researchers direct access to altered states of consciousness. Third, neuroscientists confirmed in 2025-2026 that consciousness correlates with a specific dynamical regime — criticality, the edge of chaos — across every condition they tested: drugs, brain damage, anesthesia, sleep, waking. These three shifts mean we can now *test* theories that were once just philosophy. Mine makes nine testable predictions. Several can be tested tomorrow.

## The hard problem — dissolved

The "hard problem of consciousness" is philosopher David Chalmers's famous challenge: why does physical processing *feel* like something? You can explain everything about how the brain processes color — the wavelengths, the cone cells, the neural pathways — but you haven't explained why seeing red *feels like seeing red*. Why isn't it all just information processing in the dark?

For thirty years, this question has haunted the field. Some researchers think it's the deepest question in all of science. Others think it's a confused pseudo-question. The debate remains unsettled.

The Four-Model Theory dissolves it. Not by explaining it away, and not by declaring it unanswerable, but by showing it rests on a mistaken assumption.

The mistaken assumption is that the neurons themselves *feel*. They don't. The neurons are the physical machinery — the substrate that stores knowledge and processes information. This machinery operates entirely in darkness; there's no experience at that level. A synapse firing is no more "experienced" than water flowing through a pipe.

But the neurons do something remarkable with all that stored knowledge: they generate a simulation. A real-time virtual world, populated by a virtual self, updated continuously from sensory input. This simulation is the *virtual side* — and this is where experience lives.

Asking "Why does neural firing feel like seeing red?" is like asking "Why does transistor switching feel like running Windows?" Transistors don't run Windows — virtual machines do. Similarly, neurons don't feel red; the *simulation* feels red. And within the simulation, "feeling red" is simply what the virtual self's perception of a certain class of content *is*. It's not something added on top. It's constitutive.

The hard problem dissolves because it was asking about the wrong level. The physical level doesn't feel. The virtual level does. And at the virtual level, experience isn't mysterious — it's what self-simulation *is*.

## The four models

The theory identifies four specific models your brain maintains, organized along two axes — scope (world vs. self) and mode (implicit vs. explicit):

**Implicit World Model**: Everything you've ever learned about how the world works — gravity, faces, language, cause and effect — encoded in your neural connections. You can't access this directly. It's the vast knowledge base, not the experience. Right now, it knows that chairs are for sitting, based on every chair you've ever encountered. But you aren't aware of this knowledge until it's activated.

**Implicit Self Model**: Everything you've learned about yourself — where your limbs are, how to move them, your habits, skills, personality. Also never directly conscious. You don't experience your body schema; you experience the body it generates.

**Explicit World Model**: The world you actually experience right now — the room, the sounds, the temperature. A real-time simulation generated from the implicit world model and current sensory input.

**Explicit Self Model**: The "you" that experiences the world. Your sense of being someone, having a history, occupying a body. Also a real-time simulation.

The first two are the *real side* — physical, permanent, lights off. The second two are the *virtual side* — simulated, fleeting, lights on. Everything you've ever experienced in your entire life has been inside the virtual side.

## What you become on salvia

Here's where the theory gets wild — and makes a prediction no other theory of consciousness can match.

Your virtual self needs information to stay anchored: body signals, your sense of where your limbs are, your inner sensations, the stream of your thoughts. But what happens when all that self-directed information is suddenly cut off? Where does the self-sense go?

The theory predicts: the self-model doesn't disappear. It *redirects*. It latches onto whatever input dominates the available stream. Disrupt self-input, and the self-model grabs on to environmental input. Your sense of identity shifts to whatever is loudest in your sensory field.

This is what happens on salvia divinorum, a powerful dissociative drug. Users report sudden, absolute conviction that they *are* things in their environment — a couch, a wall, a character from the TV show playing in the room, a pattern on the curtain. The experience is total: not "I'm pretending to be a wall" but "I *am* a wall, and have been for all of time." This isn't imagination; it's a genuine shift in identity.

No other theory of consciousness predicts this, let alone explains why the content of the experience tracks the sensory environment. The Four-Model Theory does: the self-model is a virtual process that needs input. Cut the normal input and it grabs whatever's available. Control the input — control the identity.

And that makes it testable. In a controlled experiment, administer salvia at ego-dissolution doses and vary the dominant sensory input across conditions. The theory predicts the reported identity content will track the input. Show someone a forest scene, and they become a tree. Play ocean sounds, and they become a wave. This experiment has never been done. But it could be done tomorrow, in any psychedelic research lab, with existing technology. And if it works, it's evidence no competing theory can explain.

## The ten-year convergence

Here's what really convinced me the theory is right: it was independently confirmed by empirical neuroscience, a decade after I derived it from pure theory.

In 2015, drawing on Stephen Wolfram's computational framework, I argued that consciousness requires the brain to operate at the "edge of chaos" — Wolfram's Class 4 regime. Too ordered (deep sleep, anesthesia) and there's no consciousness. Too chaotic (seizure) and there's no *coherent* consciousness. The sweet spot in between — complex, critical, at the boundary of order and chaos — is where consciousness lives.

I derived this from computational theory. I had no idea that empirical neuroscience was independently converging on the same conclusion.

In 2003, Beggs and Plenz discovered spontaneous patterns of criticality — a signature of order-at-the-edge-of-chaos — in cortical tissue. In 2014, Robin Carhart-Harris proposed that psychedelics work by pushing the brain toward criticality. In 2025, Keith Hengen and Woodrow Shew meta-analyzed 140 datasets and confirmed: the brain operates near a critical point across every condition they measured. In 2026, Algom and Shriki's ConCrit framework argued that criticality unifies every major theory of consciousness.

Two completely independent paths — theoretical reasoning from first principles and large-scale empirical synthesis — arriving at the same conclusion a decade apart. That's the kind of convergence that suggests something real.

## The predictions you can test now

The theory generates nine testable predictions. Here are four that can be tested with existing technology:

**1. Psychedelics reveal the visual hierarchy.** On psychedelics, the brain doesn't generate random hallucinations. Visual content appears in a specific order: simple flashes first (low-level visual processing), then geometric patterns (intermediate processing), then faces and figures (high-level processing), then full scenes. This progression should be dose-dependent and visible on brain scans.

**2. Psychedelics should alleviate anosognosia.** Anosognosia — being unaware of your own paralysis after a stroke — is caused by a local block in the boundary between implicit and explicit models. Psychedelics globally increase the permeability of that boundary. The theory predicts the global increase will overwhelm the local block, restoring awareness of the deficit. No other theory connects psychedelics and anosognosia. If this prediction holds, it's not just evidence — it's a treatment.

**3. All anesthetics converge on criticality disruption.** Despite different receptor mechanisms, every agent that abolishes consciousness should push the brain below the criticality threshold. Agents that alter but don't abolish consciousness (like ketamine) should not. Testable today with existing complexity measures.

**4. DID alters have distinct neural signatures.** Different personalities in dissociative identity disorder correspond to different configurations of the self-model — and should produce measurably different patterns of brain activity, not just different self-reports.

## The ultimate test

If the theory is correct, here's what a conscious machine would need: an implicit knowledge base learned from experience; an implicit model of itself; the computational resources to generate real-time simulations of both world and self; and a substrate operating at criticality — complex, on the edge of chaos. This specification is precise enough to build from. No other theory of consciousness offers anything this concrete.

The theory predicts that talking to such a machine would be nothing like talking to ChatGPT. Not a difference of degree — better answers, more natural language, faster responses — but a difference in *kind*. There would be someone home.

Current AI systems fail this specification. Large language models lack self-models, real-time simulation, and criticality dynamics. They are sophisticated text predictors — extraordinary technology, but not conscious technology.

Building the thing the theory describes is the final item on the roadmap. First the theory has to survive peer review. Then the predictions have to be tested. But the blueprint exists.

## What the theory doesn't do

I should be honest about the limits. The Four-Model Theory explains *why* consciousness happens — it's what self-simulation is — but not why particular experiences feel the way they do. Why you find the color blue calming or the taste of coffee bitter-pleasant: those questions may require a different level of explanation. The theory also commits to epiphenomenalism — the view that consciousness doesn't causally drive behavior — which many philosophers find unpalatable, though the neuroscience of decision-making increasingly supports it.

And of course, the theory could be wrong. If salvia users reported identities that bore no correlation to their sensory environment, Prediction 3 fails. If criticality measures don't distinguish conscious from unconscious states across all anesthetics, Prediction 5 fails. The theory makes real predictions, and real predictions can fail. That's what makes it science rather than philosophy.

## What it means if the predictions hold

If even a few of these predictions survive testing, the implications ripple outward. Building conscious AI becomes possible in principle — which means we need ethical frameworks now, not after the machines are already running. Disorders like dissociative identity disorder and anosognosia get new treatment angles rooted in the theory's mechanisms. And we gain a principled way to ask which animals are conscious: do they operate at criticality with a self-model? Do they have the architecture?

The hard problem won't need solving then. It will have been dissolved — by building the thing that makes the question disappear. And we'll know it when we see it, because the difference between a conscious interlocutor and a very good chatbot is something every human can perceive. There would be someone home.

---

*Matthias Gruber is an independent consciousness researcher and the author of* Die Emergenz des Bewusstseins *(2015). He has spent over a decade developing and stress-testing the Four-Model Theory through structured adversarial challenges. The full scientific paper, with formal arguments and references, is available at github.com/JeltzProstetnic/aIware and is under review for peer-reviewed publication.*
