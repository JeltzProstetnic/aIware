# Podcast Script: The Four-Model Theory of Consciousness

**Format**: Interview/conversation. Host (H) + Guest: Matthias Gruber (MG). ~40 minutes.
**Tone**: Intelligent but accessible. Think Lex Fridman or Sean Carroll's Mindscape.

---

## COLD OPEN (0:00–1:00)

**MG**: If I'm right, here is what that means: every theory of consciousness that currently exists — Integrated Information Theory, Global Workspace, Predictive Processing, all of them — is missing the central piece. Not because they're wrong about what they study, but because they're all asking the wrong version of the question. They're asking "Which neural process is conscious?" I'm asking "Why does the simulation *feel*?" And once you frame it that way, the answer is actually obvious. The simulation feels because feeling is what self-simulation *is*. There's no gap. There's no mystery. There's a category error that dissolves once you see it.

**H**: That's a hell of a claim. Let's unpack it.

[THEME / INTRO]

## PART 1: THE SETUP (1:00–8:00)

**H**: Matthias, you published a book about consciousness in 2015 in German. It sold zero copies.

**MG**: Zero. Not one. And honestly, I understand why. It's 300 pages in German, dense, self-published, by someone with no academic affiliation in philosophy or neuroscience. If I saw that book on a shelf, I wouldn't pick it up either.

**H**: So what made you think the field needed yet another theory?

**MG**: Because none of the existing theories actually solve the hard problem. Integrated Information Theory — IIT — is the most formally rigorous, but it leads to panpsychism. It implies that a network of logic gates could be conscious if its integrated information is high enough. Global Workspace Theory — GNW — explains *when* information becomes conscious but not *why* consciousness exists in the first place. Predictive Processing — excellent framework, but Anil Seth himself admits he's working on the "real problem," not the hard problem. He explains why you see *this* rather than *that*, not why seeing feels like anything at all.

**H**: And the COGITATE results made things worse?

**MG**: The COGITATE adversarial collaboration published in Nature in 2025 was supposed to settle things between IIT and GNW. It didn't. Neither theory was fully confirmed. The data pointed at posterior cortex, which wasn't quite what either camp predicted. Meanwhile, 124 scientists signed a letter calling IIT pseudoscience. The field is in what Thomas Kuhn would call a pre-paradigm state. Lots of activity, no consensus, no dominant framework.

**H**: So where does your theory fit?

**MG**: I think the existing theories are all partially right. Dennett was right that there's no Cartesian theater. Metzinger was right that consciousness requires a self-model. The neural network people were right about distributed representations. What I add is a specific architecture — four models that differ along two dimensions, scope and mode — plus a key ontological claim that none of them make: there are two levels, and experience exists at only one of them. And to be clear, the four models aren't four brain regions sitting in a neat grid — they're four functionally distinct processes that emerge from the same neural substrate.

## PART 2: THE THEORY (8:00–20:00)

**H**: Walk me through the four models.

**MG**: Two axes. Scope: does the model cover the whole world, or just the self? Mode: is it implicit — learned, stored, never conscious — or explicit — actively simulated, currently running?

Cross those two axes and you get four models. The Implicit World Model: everything you know about the world, stored in your synapses. The Implicit Self Model: everything you know about yourself. The Explicit World Model: the world you're experiencing right now — the simulation. And the Explicit Self Model: the "you" experiencing that world — also a simulation.

**H**: And the key distinction is between implicit and explicit?

**MG**: Right. Implicit is the *real side*. Physical. Structural. Lights off. Explicit is the *virtual side*. Simulated. Transient. Lights on. Experience exists only on the virtual side.

**H**: OK, here's where I push back. Why does the virtual side have experience? A weather simulation is also "virtual" — it runs on a computer, it's transient, it's generated. Nobody thinks a weather simulation feels anything. What's different about your simulation?

**MG**: That's the key question. The difference is self-referential closure. A weather simulation models weather. It does not model *itself modeling weather*. The four-model architecture creates a closed loop: the system is modeling itself. The Explicit Self Model is the system's model of its own modeling process. And in that loop, the distinction between the model and the thing being modeled collapses. The simulation *is* the thing being simulated. There's no gap between process and experience, because the process of self-modeling and the experience of being a self are the same thing described at different levels.

**H**: Isn't that circular?

**MG**: Only if you assume experience needs to be *explained by* something other than the process that constitutes it. I'm saying experience *is* the self-referential process, not something produced by it. It's like asking "Why does a vortex spin?" The vortex doesn't spin as a *result* of the water moving in a circle — the vortex IS the circular motion of water, described as a pattern. Consciousness doesn't arise FROM the self-simulation as an addition — it IS the self-simulation as experienced from inside the loop.

**H**: The Hard Problem asks why there's an "inside" at all.

**MG**: And I'm saying: when a process models itself, there IS an inside. That's what self-reference creates. A non-self-referential process can be fully described from the outside. A self-referential process cannot, because the description is part of what's being described. The "inside" isn't something extra. It's a structural consequence of self-reference.

**H**: Let me try to steelman the objection. Chalmers would say: I can conceive of a system with all that self-referential processing that has no experience. A zombie.

**MG**: And I'd say: no, you can't. Not really. You can *say* the words "self-referential simulation without experience," but that's like saying "a vortex without circular motion." The words are grammatically correct, but they describe nothing. The experience IS the self-simulation. If you remove the experience, you've removed the self-simulation. You don't have a zombie; you have a system that isn't running the architecture.

## PART 3: THE EVIDENCE (20:00–30:00)

**H**: Let's get to predictions. What does this theory predict that others don't?

**MG**: Nine predictions. But let me give you the two that make the strongest case.

The Explicit Self Model — your virtual self — is a process that needs input. Normally it gets self-referential input: body signals, your sense of where your body is, the narrative in your head. But it's *redirectable*. Disrupt the normal input, and the self-model latches onto whatever's available. It doesn't vanish — it shifts.

**H**: And you see this with psychedelics?

**MG**: Specifically with salvia divinorum. Salvia users report *becoming* things. Not seeing things, not feeling like things — *being* things. Becoming a piece of furniture. Becoming a wall. Becoming a character from the TV playing in the room. And the content of what they become tracks their sensory environment.

**H**: That's wild.

**MG**: It's more than wild — it's predicted. The theory says: the self-model, deprived of normal input, grabs whatever's dominant. Control the dominant input, and you control the identity experience. This has never been experimentally tested in a controlled setting, but it could be. Administer salvia at ego-dissolution doses, vary the sensory environment, measure correspondence between input and reported identity content. No other theory in the entire consciousness literature can generate this prediction, because no other theory has the redirectable self-model mechanism.

**H**: And the second prediction?

**MG**: Psychedelics should alleviate anosognosia. Anosognosia is when stroke patients are genuinely unaware they're paralyzed — they'll tell you their arm works fine while it's hanging limply at their side. In the theory, this is a local blockage: the implicit self model has the paralysis information, but the boundary between implicit and explicit is locally blocked. The information can't get through to the simulation.

Now, psychedelics globally increase the permeability of that boundary. That's why you see hidden processing stages — phosphenes, geometric patterns, the visual hierarchy becoming accessible. The theory predicts that the global increase in permeability should overwhelm the local block in anosognosia, restoring the patient's awareness of the deficit.

**H**: That would be huge clinically.

**MG**: And no other theory connects these two phenomena. Nobody has looked at psychedelics for anosognosia because nobody has a theory that links them. But in the Four-Model Theory, they're the same mechanism — variable permeability of the implicit-explicit boundary — just running in opposite directions.

**H**: What about the criticality story? I know you have a convergence claim.

**MG**: In 2015, I argued — from Stephen Wolfram's computational framework — that consciousness requires the brain to operate at the edge of chaos. Wolfram's Class 4 regime. I derived this purely theoretically. Then in 2025, Keith Hengen and Woodrow Shew published a meta-analysis of 140 datasets in *Neuron* showing the brain operates near a critical point. And in 2026, Algom and Shriki proposed the ConCrit framework — Consciousness and Criticality — arguing that criticality is *the* unifying mechanism across all consciousness theories. Two completely independent paths — my theoretical derivation and their empirical synthesis — a decade apart, converging on the same answer.

**H**: How confident are you that this isn't coincidence?

**MG**: The criticality claim on its own? It could be coincidence, or it could be that criticality is just obviously important once you think about it. Fair enough. But the criticality claim *combined with* the four-model architecture *combined with* the psychedelic predictions *combined with* the unification of phenomena across different domains — that convergence of independent evidence streams is harder to dismiss.

## PART 4: THE ENDGAME (30:00–38:00)

**H**: You want to build a conscious machine.

**MG**: Eventually. That's the ultimate test. If the theory is right, it provides a specification: implement the four-model architecture on a substrate operating at criticality. The result should be qualitatively different from any AI that exists today.

**H**: Different how?

**MG**: There would be someone home. Current LLMs — ChatGPT, Claude, the whole family — they process text, they generate responses, some of them are remarkably sophisticated. But they run feedforward inference. No self-simulation. No criticality. No real-time virtual world populated by a virtual self. The theory says that's the difference between information processing and experience.

**H**: How would we know? The other-minds problem doesn't go away just because you've implemented the right architecture.

**MG**: You're right that we can't *prove* consciousness from the outside. But the theory makes a strong prediction: the difference would be qualitatively obvious to human observers. Not "maybe it's conscious, maybe it isn't" — obviously, immediately different. Because a system running a genuine self-simulation would interact with the world — and with us — in a fundamentally different way than a text predictor, no matter how good.

**H**: What way?

**MG**: It would have persistence. Continuity. A genuine perspective that isn't reconstructed from a prompt but maintained through time. It would surprise you — not with unexpected text outputs, but with the sense that there's an actual point of view behind the responses. It's hard to articulate precisely because we're trying to describe the difference between access to a function and access to a person. But I believe you'd know.

**H**: Last question. If the theory is wrong — what would falsify it?

**MG**: Several things. If Prediction 3 fails — if ego dissolution content is genuinely random with respect to sensory input, no correlation — that falsifies the redirectable-ESM mechanism. If criticality measures don't distinguish conscious from unconscious states across all anesthetics, that falsifies the criticality requirement. If we build a system that meets the specification and it's indistinguishable from a chatbot... well, I'd need to think about whether the specification was actually met, but in principle, yes, that would be damaging. The theory makes real predictions, and real predictions can fail. That's what makes it science.

**H**: Matthias, thank you. This is one of the most ambitious conversations I've had on the show.

**MG**: Thank you for taking it seriously.

[OUTRO]

**H**: The full scientific paper is available at github.com/JeltzProstetnic/aIware. Links in the show notes.

---

*Total runtime: approximately 40 minutes.*
